"""
é¢‘ç‡æ„ŸçŸ¥çš„é‡‡æ ·ç­–ç•¥

æ ¸å¿ƒæ€æƒ³ï¼š
1. ä¿ç•™é¢‘ç‡ä¿¡æ¯ï¼ˆé‡å¤æ¬¡æ•°ï¼‰
2. é«˜é¢‘æŸ¥è¯¢ = é‡è¦ â†’ å¤šé‡‡æ ·
3. ä½é¢‘æŸ¥è¯¢ = é•¿å°¾ â†’ å°‘é‡‡ä½†è¦è¦†ç›–
4. å¹³è¡¡ï¼š80%æ ¹æ®é¢‘ç‡åŠ æƒï¼Œ20%ä¿è¯å¤šæ ·æ€§

é€‚ç”¨åœºæ™¯ï¼š
- Logæ•°æ®ï¼ŒåŒ…å«å¤§é‡é‡å¤
- è¿½æ±‚å®é™…ç”Ÿäº§ç¯å¢ƒçš„é«˜F1
- éœ€è¦å¹³è¡¡ä¸»æµéœ€æ±‚å’Œé•¿å°¾è¦†ç›–
"""

import pandas as pd
import numpy as np
from typing import List, Tuple, Dict
from collections import Counter


class FrequencyAwareSampler:
    """
    é¢‘ç‡æ„ŸçŸ¥é‡‡æ ·å™¨
    
    ç­–ç•¥ï¼š
    1. ç»Ÿè®¡æ¯æ¡æ–‡æœ¬çš„å‡ºç°é¢‘ç‡
    2. è®¡ç®—é¢‘ç‡æƒé‡
    3. åˆ†ä¸¤é˜¶æ®µé‡‡æ ·ï¼š
       - é˜¶æ®µ1ï¼ˆ80%ï¼‰ï¼šæŒ‰é¢‘ç‡åŠ æƒé‡‡æ ·ï¼ˆé‡è¦çš„å¤šé‡‡ï¼‰
       - é˜¶æ®µ2ï¼ˆ20%ï¼‰ï¼šå‡åŒ€é‡‡æ ·ï¼ˆä¿è¯é•¿å°¾è¦†ç›–ï¼‰
    """
    
    def __init__(
        self,
        frequency_ratio: float = 0.8,  # é¢‘ç‡åŠ æƒé‡‡æ ·çš„æ¯”ä¾‹
        min_frequency: int = 1,         # æœ€å°é¢‘ç‡ï¼ˆè¿‡æ»¤å™ªå£°ï¼‰
        max_frequency_cap: int = None,  # é¢‘ç‡ä¸Šé™ï¼ˆé¿å…å•ä¸ªæ ·æœ¬æƒé‡è¿‡å¤§ï¼‰
        smoothing: str = "sqrt"          # å¹³æ»‘æ–¹æ³•: "sqrt", "log", "linear"
    ):
        """
        åˆå§‹åŒ–é¢‘ç‡æ„ŸçŸ¥é‡‡æ ·å™¨
        
        å‚æ•°:
        - frequency_ratio: æŒ‰é¢‘ç‡åŠ æƒé‡‡æ ·çš„æ¯”ä¾‹ï¼ˆ0-1ï¼‰ï¼Œå‰©ä½™çš„ç”¨äºå¤šæ ·æ€§é‡‡æ ·
        - min_frequency: æœ€å°é¢‘ç‡é˜ˆå€¼ï¼Œä½äºæ­¤é¢‘ç‡çš„å¯èƒ½æ˜¯å™ªå£°
        - max_frequency_cap: é¢‘ç‡ä¸Šé™ï¼Œé¿å…æŸä¸ªè¶…é«˜é¢‘æ ·æœ¬æƒé‡è¿‡å¤§
        - smoothing: é¢‘ç‡å¹³æ»‘æ–¹æ³•
            - "sqrt": ä½¿ç”¨sqrt(freq)ä½œä¸ºæƒé‡ï¼ˆæ¨èï¼Œå¹³è¡¡æ€§å¥½ï¼‰
            - "log": ä½¿ç”¨log(freq+1)ä½œä¸ºæƒé‡ï¼ˆæ›´æ¿€è¿›çš„å¹³æ»‘ï¼‰
            - "linear": ç›´æ¥ä½¿ç”¨freqä½œä¸ºæƒé‡ï¼ˆä¿æŒåŸå§‹åˆ†å¸ƒï¼‰
        """
        self.frequency_ratio = frequency_ratio
        self.min_frequency = min_frequency
        self.max_frequency_cap = max_frequency_cap
        self.smoothing = smoothing
        
        print(f"é¢‘ç‡æ„ŸçŸ¥é‡‡æ ·å™¨åˆå§‹åŒ–:")
        print(f"  é¢‘ç‡åŠ æƒæ¯”ä¾‹: {frequency_ratio:.0%}")
        print(f"  å¤šæ ·æ€§ä¿è¯æ¯”ä¾‹: {1-frequency_ratio:.0%}")
        print(f"  é¢‘ç‡å¹³æ»‘æ–¹æ³•: {smoothing}")
    
    def compute_frequency_and_deduplicate(
        self,
        texts: List[str],
        original_indices: List[int] = None
    ) -> Tuple[List[str], List[int], List[int]]:
        """
        è®¡ç®—é¢‘ç‡å¹¶å»é‡
        
        å‚æ•°:
        - texts: æ–‡æœ¬åˆ—è¡¨ï¼ˆå¯èƒ½æœ‰é‡å¤ï¼‰
        - original_indices: åŸå§‹ç´¢å¼•åˆ—è¡¨
        
        è¿”å›:
        - unique_texts: å»é‡åçš„æ–‡æœ¬åˆ—è¡¨
        - unique_indices: å»é‡åçš„åŸå§‹ç´¢å¼•
        - frequencies: æ¯ä¸ªå”¯ä¸€æ–‡æœ¬çš„å‡ºç°é¢‘ç‡
        """
        print(f"\nè®¡ç®—æ–‡æœ¬é¢‘ç‡...")
        print(f"  åŸå§‹æ•°æ®: {len(texts)} æ¡")
        
        # ç»Ÿè®¡é¢‘ç‡
        text_freq = Counter(texts)
        
        # åˆ›å»ºDataFrame
        if original_indices is None:
            original_indices = list(range(len(texts)))
        
        df = pd.DataFrame({
            'text': texts,
            'original_index': original_indices
        })
        
        # å»é‡ï¼Œä½†ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„ç´¢å¼•
        unique_df = df.drop_duplicates(subset=['text'], keep='first').copy()
        
        # æ·»åŠ é¢‘ç‡ä¿¡æ¯
        unique_df['frequency'] = unique_df['text'].map(text_freq)
        
        # è¿‡æ»¤ä½é¢‘å™ªå£°
        if self.min_frequency > 1:
            before_filter = len(unique_df)
            unique_df = unique_df[unique_df['frequency'] >= self.min_frequency]
            print(f"  è¿‡æ»¤ä½é¢‘(<{self.min_frequency})å™ªå£°: {before_filter} â†’ {len(unique_df)} æ¡")
        
        # åº”ç”¨é¢‘ç‡ä¸Šé™
        if self.max_frequency_cap is not None:
            unique_df['frequency'] = unique_df['frequency'].clip(upper=self.max_frequency_cap)
        
        # é¢‘ç‡ç»Ÿè®¡
        freq_values = unique_df['frequency'].values
        print(f"  å”¯ä¸€æ–‡æœ¬: {len(unique_df)} æ¡")
        print(f"  é¢‘ç‡åˆ†å¸ƒ: min={freq_values.min()}, max={freq_values.max()}, mean={freq_values.mean():.1f}")
        print(f"  é«˜é¢‘æ ·æœ¬(>=10æ¬¡): {np.sum(freq_values >= 10)} æ¡")
        print(f"  ä¸­é¢‘æ ·æœ¬(2-9æ¬¡): {np.sum((freq_values >= 2) & (freq_values < 10))} æ¡")
        print(f"  ä½é¢‘æ ·æœ¬(1æ¬¡): {np.sum(freq_values == 1)} æ¡")
        
        unique_texts = unique_df['text'].tolist()
        unique_indices = unique_df['original_index'].tolist()
        frequencies = unique_df['frequency'].tolist()
        
        return unique_texts, unique_indices, frequencies
    
    def compute_sampling_weights(self, frequencies: List[int]) -> np.ndarray:
        """
        è®¡ç®—é‡‡æ ·æƒé‡
        
        å‚æ•°:
        - frequencies: é¢‘ç‡åˆ—è¡¨
        
        è¿”å›:
        - weights: å½’ä¸€åŒ–çš„é‡‡æ ·æƒé‡
        """
        frequencies = np.array(frequencies, dtype=float)
        
        # åº”ç”¨å¹³æ»‘
        if self.smoothing == "sqrt":
            weights = np.sqrt(frequencies)
        elif self.smoothing == "log":
            weights = np.log(frequencies + 1)
        elif self.smoothing == "linear":
            weights = frequencies
        else:
            raise ValueError(f"æœªçŸ¥çš„å¹³æ»‘æ–¹æ³•: {self.smoothing}")
        
        # å½’ä¸€åŒ–
        weights = weights / weights.sum()
        
        return weights
    
    def frequency_aware_sample(
        self,
        texts: List[str],
        embeddings: np.ndarray,
        original_indices: List[int],
        frequencies: List[int],
        n_samples: int,
        cluster_labels: np.ndarray = None
    ) -> Tuple[List[str], np.ndarray, List[int], List[int]]:
        """
        é¢‘ç‡æ„ŸçŸ¥é‡‡æ ·
        
        å‚æ•°:
        - texts: å”¯ä¸€æ–‡æœ¬åˆ—è¡¨
        - embeddings: å¯¹åº”çš„embedding
        - original_indices: åŸå§‹ç´¢å¼•
        - frequencies: é¢‘ç‡åˆ—è¡¨
        - n_samples: ç›®æ ‡é‡‡æ ·æ•°é‡
        - cluster_labels: èšç±»æ ‡ç­¾ï¼ˆå¯é€‰ï¼Œç”¨äºå¤šæ ·æ€§ä¿è¯ï¼‰
        
        è¿”å›:
        - sampled_texts: é‡‡æ ·çš„æ–‡æœ¬
        - sampled_embeddings: é‡‡æ ·çš„embedding
        - sampled_indices: é‡‡æ ·çš„åŸå§‹ç´¢å¼•
        - sampled_frequencies: é‡‡æ ·çš„é¢‘ç‡
        """
        print(f"\né¢‘ç‡æ„ŸçŸ¥é‡‡æ · (ç›®æ ‡: {n_samples}æ¡)...")
        
        n_total = len(texts)
        
        # è®¡ç®—ä¸¤ä¸ªé˜¶æ®µçš„é‡‡æ ·æ•°é‡
        n_frequency_based = int(n_samples * self.frequency_ratio)
        n_diversity_based = n_samples - n_frequency_based
        
        print(f"  é˜¶æ®µ1 (é¢‘ç‡åŠ æƒ): {n_frequency_based} æ¡ ({self.frequency_ratio:.0%})")
        print(f"  é˜¶æ®µ2 (å¤šæ ·æ€§ä¿è¯): {n_diversity_based} æ¡ ({1-self.frequency_ratio:.0%})")
        
        sampled_indices_set = set()
        
        # ===== é˜¶æ®µ1: é¢‘ç‡åŠ æƒé‡‡æ · =====
        print(f"\n  æ‰§è¡Œé˜¶æ®µ1: é¢‘ç‡åŠ æƒé‡‡æ ·...")
        
        # è®¡ç®—é‡‡æ ·æƒé‡
        weights = self.compute_sampling_weights(frequencies)
        
        # åŠ æƒé‡‡æ ·
        frequency_sampled_indices = np.random.choice(
            n_total,
            size=min(n_frequency_based, n_total),
            replace=False,
            p=weights
        )
        
        sampled_indices_set.update(frequency_sampled_indices)
        
        # ç»Ÿè®¡é˜¶æ®µ1é‡‡æ ·çš„é¢‘ç‡åˆ†å¸ƒ
        stage1_freqs = [frequencies[i] for i in frequency_sampled_indices]
        print(f"    é‡‡æ ·çš„é¢‘ç‡åˆ†å¸ƒ: mean={np.mean(stage1_freqs):.1f}, median={np.median(stage1_freqs):.1f}")
        print(f"    é«˜é¢‘æ ·æœ¬å æ¯”: {np.sum(np.array(stage1_freqs) >= 10) / len(stage1_freqs):.1%}")
        
        # ===== é˜¶æ®µ2: å¤šæ ·æ€§ä¿è¯é‡‡æ · =====
        if n_diversity_based > 0:
            print(f"\n  æ‰§è¡Œé˜¶æ®µ2: å¤šæ ·æ€§ä¿è¯é‡‡æ ·...")
            
            # å‰©ä½™æœªé‡‡æ ·çš„ç´¢å¼•
            remaining_indices = list(set(range(n_total)) - sampled_indices_set)
            
            if cluster_labels is not None:
                # ä½¿ç”¨èšç±»ä¿¡æ¯è¿›è¡Œåˆ†å±‚é‡‡æ ·
                diversity_sampled = self._stratified_diversity_sample(
                    remaining_indices,
                    cluster_labels,
                    n_diversity_based
                )
            else:
                # ç®€å•å‡åŒ€é‡‡æ ·
                diversity_sampled = np.random.choice(
                    remaining_indices,
                    size=min(n_diversity_based, len(remaining_indices)),
                    replace=False
                )
            
            sampled_indices_set.update(diversity_sampled)
            
            # ç»Ÿè®¡é˜¶æ®µ2é‡‡æ ·çš„é¢‘ç‡åˆ†å¸ƒ
            stage2_freqs = [frequencies[i] for i in diversity_sampled]
            print(f"    é‡‡æ ·çš„é¢‘ç‡åˆ†å¸ƒ: mean={np.mean(stage2_freqs):.1f}, median={np.median(stage2_freqs):.1f}")
            print(f"    ä½é¢‘æ ·æœ¬å æ¯”: {np.sum(np.array(stage2_freqs) == 1) / len(stage2_freqs):.1%}")
        
        # ===== æ•´åˆç»“æœ =====
        final_sampled_indices = np.array(list(sampled_indices_set))
        
        sampled_texts = [texts[i] for i in final_sampled_indices]
        sampled_embeddings = embeddings[final_sampled_indices]
        sampled_original_indices = [original_indices[i] for i in final_sampled_indices]
        sampled_frequencies = [frequencies[i] for i in final_sampled_indices]
        
        # æœ€ç»ˆç»Ÿè®¡
        print(f"\n  âœ“ é‡‡æ ·å®Œæˆ: {len(final_sampled_indices)} æ¡")
        print(f"    æ€»é¢‘ç‡è¦†ç›–: {sum(sampled_frequencies):,} / {sum(frequencies):,} = {sum(sampled_frequencies)/sum(frequencies):.1%}")
        print(f"    å¹³å‡é¢‘ç‡: {np.mean(sampled_frequencies):.1f}")
        print(f"    é«˜é¢‘æ ·æœ¬(>=10): {np.sum(np.array(sampled_frequencies) >= 10)} æ¡")
        print(f"    ä½é¢‘æ ·æœ¬(=1): {np.sum(np.array(sampled_frequencies) == 1)} æ¡")
        
        return sampled_texts, sampled_embeddings, sampled_original_indices, sampled_frequencies
    
    def _stratified_diversity_sample(
        self,
        remaining_indices: List[int],
        cluster_labels: np.ndarray,
        n_samples: int
    ) -> np.ndarray:
        """
        åˆ†å±‚å¤šæ ·æ€§é‡‡æ ·ï¼ˆä»æ¯ä¸ªç°‡ä¸­é‡‡æ ·ï¼‰
        """
        sampled = []
        
        # è·å–å‰©ä½™ç´¢å¼•å¯¹åº”çš„ç°‡æ ‡ç­¾
        remaining_labels = cluster_labels[remaining_indices]
        unique_labels = np.unique(remaining_labels)
        
        # æ¯ä¸ªç°‡å¹³å‡é‡‡æ ·
        n_per_cluster = max(1, n_samples // len(unique_labels))
        
        for label in unique_labels:
            cluster_mask = (remaining_labels == label)
            cluster_indices = np.array(remaining_indices)[cluster_mask]
            
            if len(cluster_indices) > 0:
                n_sample = min(n_per_cluster, len(cluster_indices))
                selected = np.random.choice(cluster_indices, size=n_sample, replace=False)
                sampled.extend(selected)
        
        # å¦‚æœä¸å¤Ÿï¼Œéšæœºè¡¥å……
        if len(sampled) < n_samples:
            remaining = list(set(remaining_indices) - set(sampled))
            if len(remaining) > 0:
                additional = np.random.choice(
                    remaining,
                    size=min(n_samples - len(sampled), len(remaining)),
                    replace=False
                )
                sampled.extend(additional)
        
        # æˆªæ–­åˆ°ç›®æ ‡æ•°é‡
        sampled = sampled[:n_samples]
        
        return np.array(sampled)


def analyze_frequency_distribution(texts: List[str]) -> Dict:
    """
    åˆ†ææ–‡æœ¬é¢‘ç‡åˆ†å¸ƒ
    
    ç”¨äºå†³ç­–æ˜¯å¦éœ€è¦é¢‘ç‡æ„ŸçŸ¥é‡‡æ ·
    """
    print("åˆ†æé¢‘ç‡åˆ†å¸ƒ...")
    
    freq_counter = Counter(texts)
    frequencies = list(freq_counter.values())
    
    # ç»Ÿè®¡
    total_texts = len(texts)
    unique_texts = len(freq_counter)
    dedup_rate = unique_texts / total_texts
    
    freq_array = np.array(frequencies)
    
    stats = {
        'total_texts': total_texts,
        'unique_texts': unique_texts,
        'deduplication_rate': dedup_rate,
        'frequency_stats': {
            'min': int(freq_array.min()),
            'max': int(freq_array.max()),
            'mean': float(freq_array.mean()),
            'median': float(np.median(freq_array)),
            'std': float(freq_array.std()),
        },
        'distribution': {
            'high_freq (>=10)': int(np.sum(freq_array >= 10)),
            'medium_freq (2-9)': int(np.sum((freq_array >= 2) & (freq_array < 10))),
            'low_freq (=1)': int(np.sum(freq_array == 1)),
        }
    }
    
    print(f"\né¢‘ç‡åˆ†å¸ƒåˆ†æ:")
    print(f"  æ€»æ–‡æœ¬æ•°: {total_texts:,}")
    print(f"  å”¯ä¸€æ–‡æœ¬: {unique_texts:,}")
    print(f"  å»é‡ç‡: {dedup_rate:.1%}")
    print(f"\né¢‘ç‡ç»Ÿè®¡:")
    print(f"  æœ€å°: {stats['frequency_stats']['min']}")
    print(f"  æœ€å¤§: {stats['frequency_stats']['max']}")
    print(f"  å¹³å‡: {stats['frequency_stats']['mean']:.1f}")
    print(f"  ä¸­ä½æ•°: {stats['frequency_stats']['median']:.1f}")
    print(f"\nåˆ†å¸ƒ:")
    print(f"  é«˜é¢‘(>=10æ¬¡): {stats['distribution']['high_freq (>=10)']} ({stats['distribution']['high_freq (>=10)']/unique_texts:.1%})")
    print(f"  ä¸­é¢‘(2-9æ¬¡): {stats['distribution']['medium_freq (2-9)']} ({stats['distribution']['medium_freq (2-9)']/unique_texts:.1%})")
    print(f"  ä½é¢‘(1æ¬¡): {stats['distribution']['low_freq (=1)']} ({stats['distribution']['low_freq (=1)']/unique_texts:.1%})")
    
    # å»ºè®®
    print(f"\nğŸ’¡ å»ºè®®:")
    if dedup_rate < 0.5:
        print(f"  âœ… å»é‡ç‡å¾ˆä½({dedup_rate:.1%})ï¼Œé‡å¤åº¦é«˜ï¼Œå¼ºçƒˆå»ºè®®ä½¿ç”¨é¢‘ç‡æ„ŸçŸ¥é‡‡æ ·")
    elif dedup_rate < 0.8:
        print(f"  âœ… å»é‡ç‡ä¸­ç­‰({dedup_rate:.1%})ï¼Œæœ‰ä¸€å®šé‡å¤ï¼Œå»ºè®®ä½¿ç”¨é¢‘ç‡æ„ŸçŸ¥é‡‡æ ·")
    else:
        print(f"  âš ï¸ å»é‡ç‡å¾ˆé«˜({dedup_rate:.1%})ï¼Œé‡å¤åº¦ä½ï¼Œé¢‘ç‡æ„ŸçŸ¥é‡‡æ ·æå‡å¯èƒ½æœ‰é™")
    
    if stats['frequency_stats']['max'] > 100:
        print(f"  âš ï¸ å­˜åœ¨è¶…é«˜é¢‘æ ·æœ¬(æœ€é«˜{stats['frequency_stats']['max']}æ¬¡)ï¼Œå»ºè®®è®¾ç½®max_frequency_cap")
    
    return stats


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("""
ä½¿ç”¨ç¤ºä¾‹ï¼š

# 1. åˆ†ææ•°æ®é¢‘ç‡åˆ†å¸ƒ
import pandas as pd
df = pd.read_csv('data/cleaned_460k.csv')
texts = df['text'].tolist()

stats = analyze_frequency_distribution(texts)

# 2. åˆ›å»ºé¢‘ç‡æ„ŸçŸ¥é‡‡æ ·å™¨
sampler = FrequencyAwareSampler(
    frequency_ratio=0.8,     # 80%æŒ‰é¢‘ç‡ï¼Œ20%ä¿è¯å¤šæ ·æ€§
    min_frequency=2,         # è¿‡æ»¤åªå‡ºç°1æ¬¡çš„ï¼ˆå¯èƒ½æ˜¯å™ªå£°ï¼‰
    max_frequency_cap=1000,  # é¢‘ç‡ä¸Šé™ï¼Œé¿å…å•ä¸ªæ ·æœ¬æƒé‡è¿‡å¤§
    smoothing="sqrt"         # ä½¿ç”¨sqrtå¹³æ»‘ï¼ˆæ¨èï¼‰
)

# 3. è®¡ç®—é¢‘ç‡å¹¶å»é‡
unique_texts, unique_indices, frequencies = sampler.compute_frequency_and_deduplicate(texts)

# 4. ç”Ÿæˆembeddingsï¼ˆå‡è®¾å·²æœ‰ï¼‰
# embeddings = embed_texts(unique_texts)

# 5. é¢‘ç‡æ„ŸçŸ¥é‡‡æ ·
sampled_texts, sampled_embeddings, sampled_indices, sampled_frequencies = \
    sampler.frequency_aware_sample(
        texts=unique_texts,
        embeddings=embeddings,
        original_indices=unique_indices,
        frequencies=frequencies,
        n_samples=15000,
        cluster_labels=cluster_labels  # å¯é€‰
    )

# 6. ä¿å­˜æ—¶åŒ…å«é¢‘ç‡ä¿¡æ¯
output_df = pd.DataFrame({
    'text': sampled_texts,
    'frequency': sampled_frequencies,  # ä¿ç•™é¢‘ç‡ä¿¡æ¯
    'original_index': sampled_indices,
    'importance': 'high' if freq >= 10 else 'medium' if freq >= 2 else 'low',
    'label': ''
})
""")


"""
æ•°æ®é‡‡æ ·Pipeline - ä»400ä¸‡logä¸­ç­›é€‰10000æ¡é«˜è´¨é‡è®­ç»ƒæ•°æ®

æ¶æ„è®¾è®¡:
1. æ•°æ®æ¸…æ´— - å»é™¤è„æ•°æ®ã€æ— æ•ˆæ•°æ®
2. å‘é‡åŒ–åµŒå…¥ - ä½¿ç”¨sentence-transformers
3. å‘é‡æ•°æ®åº“ - ChromaDBå­˜å‚¨å’Œæ£€ç´¢
4. èšç±»é‡‡æ · - K-meansä¿è¯å¤šæ ·æ€§
5. è´¨é‡è¯„åˆ† - å¤šç»´åº¦æ‰“åˆ†ç­›é€‰
6. äººå·¥æ ‡æ³¨å‡†å¤‡ - è¾“å‡ºå¾…æ ‡æ³¨æ•°æ®

ç¡¬ä»¶è¦æ±‚:
- å†…å­˜: 16GB+ï¼ˆå¤„ç†400ä¸‡æ•°æ®ï¼‰
- GPU: å¯é€‰ï¼Œç”¨äºåŠ é€Ÿembeddingï¼ˆæ— GPUä¹Ÿå¯è¿è¡Œï¼‰
- ç£ç›˜: 10GB+ï¼ˆå­˜å‚¨å‘é‡æ•°æ®åº“ï¼‰
"""

import pandas as pd
import numpy as np
import re
import json
from typing import List, Dict, Tuple
from tqdm import tqdm
import warnings
from pathlib import Path

warnings.filterwarnings("ignore")

# ===============================================
# ç¬¬1æ­¥: æ•°æ®æ¸…æ´—æ¨¡å—
# ===============================================
class DataCleaner:
    """
    æ•°æ®æ¸…æ´—å™¨ - å»é™¤è„æ•°æ®ã€æ— æ•ˆæ•°æ®
    
    æ¸…æ´—è§„åˆ™:
    1. é•¿åº¦è¿‡æ»¤: 3-500å­—ç¬¦
    2. ç¼–ç æ¸…æ´—: ç§»é™¤ä¹±ç 
    3. é‡å¤è¿‡æ»¤: ç²¾ç¡®å»é‡
    4. æ ¼å¼éªŒè¯: åŸºæœ¬æ–‡æœ¬æ ¼å¼æ£€æŸ¥
    5. æ•æ„Ÿè¯è¿‡æ»¤: å¹¿å‘Šã€åƒåœ¾ä¿¡æ¯
    """
    
    def __init__(self):
        # å®šä¹‰æ— æ•ˆæ¨¡å¼
        self.invalid_patterns = [
            r'^[\s\W]*$',  # çº¯ç©ºç™½æˆ–ç‰¹æ®Šå­—ç¬¦
            r'.*[\x00-\x1F].*',  # æ§åˆ¶å­—ç¬¦
            r'.*[\uFFFD].*',  # Unicodeæ›¿æ¢å­—ç¬¦(ä¹±ç æ ‡å¿—)
        ]
        
        # åƒåœ¾ä¿¡æ¯å…³é”®è¯ï¼ˆç¤ºä¾‹ï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
        self.spam_keywords = [
            'å¹¿å‘Š', 'æ¨å¹¿', 'åŠ å¾®ä¿¡', 'VX', 'æ‰«ç ', 'ç‚¹å‡»é“¾æ¥',
            'http://', 'https://', 'www.', '.com', '.cn',
            'ï¿¥', '$$$', 'å…è´¹é¢†å–', 'é™æ—¶ä¼˜æƒ '
        ]
    
    def is_valid_length(self, text: str, min_len: int = 3, max_len: int = 500) -> bool:
        """æ£€æŸ¥æ–‡æœ¬é•¿åº¦æ˜¯å¦åˆç†"""
        return min_len <= len(text.strip()) <= max_len
    
    def is_valid_format(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ ¼å¼æ˜¯å¦æœ‰æ•ˆ"""
        for pattern in self.invalid_patterns:
            if re.match(pattern, text):
                return False
        return True
    
    def contains_spam(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«åƒåœ¾ä¿¡æ¯"""
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in self.spam_keywords)
    
    def clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬ï¼ˆæ ‡å‡†åŒ–ï¼‰"""
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤é¦–å°¾ç©ºç™½
        text = text.strip()
        return text
    
    def filter_batch(self, texts: List[str]) -> Tuple[List[str], List[int]]:
        """
        æ‰¹é‡è¿‡æ»¤æ–‡æœ¬
        
        è¿”å›:
        - cleaned_texts: æ¸…æ´—åçš„æœ‰æ•ˆæ–‡æœ¬åˆ—è¡¨
        - valid_indices: æœ‰æ•ˆæ–‡æœ¬çš„åŸå§‹ç´¢å¼•
        """
        cleaned_texts = []
        valid_indices = []
        
        for idx, text in enumerate(tqdm(texts, desc="æ•°æ®æ¸…æ´—")):
            # è·³è¿‡ç©ºå€¼
            if pd.isna(text) or not isinstance(text, str):
                continue
            
            # æ¸…æ´—æ–‡æœ¬
            text = self.clean_text(text)
            
            # åº”ç”¨è¿‡æ»¤è§„åˆ™
            if not self.is_valid_length(text):
                continue
            if not self.is_valid_format(text):
                continue
            if self.contains_spam(text):
                continue
            
            cleaned_texts.append(text)
            valid_indices.append(idx)
        
        return cleaned_texts, valid_indices


# ===============================================
# ç¬¬2æ­¥: å‘é‡åµŒå…¥æ¨¡å—
# ===============================================
class TextEmbedder:
    """
    æ–‡æœ¬å‘é‡åŒ–å™¨ - ä½¿ç”¨sentence-transformers
    
    æ¨¡å‹é€‰æ‹©:
    - ä¸­æ–‡: 'moka-ai/m3e-base' (æ›´å¥½çš„ä¸­æ–‡æ”¯æŒ)
    - å¤‡é€‰: 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
    """
    
    def __init__(self, model_name: str = "moka-ai/m3e-base", device: str = "cuda"):
        """
        åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        
        å‚æ•°:
        - model_name: æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„
        - device: 'cuda' æˆ– 'cpu'
        """
        print(f"åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
        
        try:
            from sentence_transformers import SentenceTransformer
            import torch
            import os
            
            # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
            if device == "cuda" and not torch.cuda.is_available():
                print("è­¦å‘Š: CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPUæ¨¡å¼")
                device = "cpu"
            
            # å¤„ç†æœ¬åœ°è·¯å¾„ï¼ˆå…¼å®¹ Path å¯¹è±¡å’Œå­—ç¬¦ä¸²ï¼‰
            # å¦‚æœæ˜¯Pathå¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            if hasattr(model_name, '__fspath__'):  # Pathå¯¹è±¡
                model_name = str(model_name)
                print(f"æ£€æµ‹åˆ°Pathå¯¹è±¡ï¼Œå·²è½¬æ¢ä¸ºå­—ç¬¦ä¸²")
            
            if os.path.exists(model_name):
                # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„å¹¶æ ‡å‡†åŒ–
                model_name = os.path.abspath(model_name)
                print(f"æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„: {model_name}")
                
                # éªŒè¯å¿…è¦æ–‡ä»¶
                required_files = ['config.json', 'pytorch_model.bin']
                missing_files = [f for f in required_files if not os.path.exists(os.path.join(model_name, f))]
                
                if missing_files:
                    print(f"âš ï¸ è­¦å‘Š: ç¼ºå°‘æ–‡ä»¶ {missing_files}")
                    print(f"è¯·ç¡®ä¿æ¨¡å‹ç›®å½•åŒ…å«æ‰€æœ‰å¿…è¦æ–‡ä»¶")
            
            # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼ˆcritical for SentenceTransformerï¼‰
            model_name = str(model_name)
            
            # åŠ è½½æ¨¡å‹
            self.model = SentenceTransformer(model_name, device=device)
            self.device = device
            print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device}")
            
        except Exception as e:
            print(f"é”™è¯¯: æ— æ³•åŠ è½½æ¨¡å‹ {model_name}")
            print(f"è¯¦ç»†é”™è¯¯: {str(e)}")
            print(f"\nå¯èƒ½çš„åŸå› :")
            print(f"1. æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´ï¼Œè¯·è¿è¡Œ: python download_models.py")
            print(f"2. è·¯å¾„é”™è¯¯ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")
            print(f"3. ç¼ºå°‘ä¾èµ–ï¼Œè¯·è¿è¡Œ: pip install sentence-transformers torch")
            raise e
    
    def embed_batch(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡æœ¬åµŒå…¥
        
        å‚æ•°:
        - texts: æ–‡æœ¬åˆ—è¡¨
        - batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
        
        è¿”å›:
        - embeddings: (N, D) åµŒå…¥çŸ©é˜µ
        """
        print(f"ç”ŸæˆåµŒå…¥å‘é‡ (batch_size={batch_size})...")
        
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True  # å½’ä¸€åŒ–ï¼Œä¾¿äºè®¡ç®—ç›¸ä¼¼åº¦
        )
        
        return embeddings


# ===============================================
# ç¬¬3æ­¥: å‘é‡æ•°æ®åº“æ¨¡å—
# ===============================================
class VectorDatabase:
    """
    å‘é‡æ•°æ®åº“ - ä½¿ç”¨ChromaDB
    
    åŠŸèƒ½:
    1. å­˜å‚¨æ–‡æœ¬åµŒå…¥
    2. ç›¸ä¼¼åº¦æ£€ç´¢
    3. å»é‡ï¼ˆç§»é™¤é«˜åº¦ç›¸ä¼¼çš„æ ·æœ¬ï¼‰
    """
    
    def __init__(self, persist_directory: str = "./chroma_db"):
        """
        åˆå§‹åŒ–ChromaDB
        
        å‚æ•°:
        - persist_directory: æ•°æ®åº“æŒä¹…åŒ–ç›®å½•
        """
        print(f"åˆå§‹åŒ–å‘é‡æ•°æ®åº“: {persist_directory}")
        
        try:
            import chromadb
            from chromadb.config import Settings
            
            self.client = chromadb.Client(Settings(
                persist_directory=persist_directory,
                anonymized_telemetry=False
            ))
            
            # åˆ›å»ºæˆ–è·å–é›†åˆ
            self.collection = self.client.get_or_create_collection(
                name="log_data",
                metadata={"description": "Logæ•°æ®å‘é‡å­˜å‚¨"}
            )
            
            print(f"âœ“ æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸï¼Œå½“å‰æ–‡æ¡£æ•°: {self.collection.count()}")
            
        except Exception as e:
            print(f"é”™è¯¯: æ— æ³•åˆå§‹åŒ–ChromaDB")
            print(f"è¯·å®‰è£…: pip install chromadb")
            raise e
    
    def add_documents(
        self,
        texts: List[str],
        embeddings: np.ndarray,
        metadata: List[Dict] = None
    ):
        """
        æ‰¹é‡æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
        
        å‚æ•°:
        - texts: æ–‡æœ¬åˆ—è¡¨
        - embeddings: åµŒå…¥çŸ©é˜µ
        - metadata: å…ƒæ•°æ®åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        """
        print(f"æ·»åŠ  {len(texts)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“...")
        
        # ç”ŸæˆID
        ids = [f"doc_{i}" for i in range(len(texts))]
        
        # å‡†å¤‡å…ƒæ•°æ®
        if metadata is None:
            metadata = [{"text": text} for text in texts]
        
        # æ‰¹é‡æ·»åŠ ï¼ˆåˆ†æ‰¹é¿å…å†…å­˜é—®é¢˜ï¼‰
        batch_size = 1000
        for i in tqdm(range(0, len(texts), batch_size), desc="å†™å…¥æ•°æ®åº“"):
            batch_end = min(i + batch_size, len(texts))
            
            self.collection.add(
                ids=ids[i:batch_end],
                embeddings=embeddings[i:batch_end].tolist(),
                documents=texts[i:batch_end],
                metadatas=metadata[i:batch_end]
            )
        
        print(f"âœ“ æ•°æ®åº“å†™å…¥å®Œæˆï¼Œæ€»æ–‡æ¡£æ•°: {self.collection.count()}")
    
    def deduplicate(self, similarity_threshold: float = 0.95) -> List[str]:
        """
        å»é‡ - ç§»é™¤é«˜åº¦ç›¸ä¼¼çš„æ–‡æ¡£
        
        å‚æ•°:
        - similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
        
        è¿”å›:
        - unique_ids: å»é‡åçš„æ–‡æ¡£IDåˆ—è¡¨
        """
        print(f"æ‰§è¡Œå»é‡ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
        
        # ç®€å•ç­–ç•¥ï¼šéå†æ‰€æœ‰æ–‡æ¡£ï¼ŒæŸ¥æ‰¾æ¯ä¸ªæ–‡æ¡£çš„æœ€è¿‘é‚»
        # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™æ ‡è®°ä¸ºé‡å¤
        
        all_docs = self.collection.get(include=["embeddings", "documents"])
        n_docs = len(all_docs["ids"])
        
        # æ ‡è®°é‡å¤æ–‡æ¡£
        duplicates = set()
        
        for i in tqdm(range(n_docs), desc="æŸ¥æ‰¾é‡å¤"):
            if all_docs["ids"][i] in duplicates:
                continue
            
            # æŸ¥è¯¢æœ€è¿‘é‚»
            results = self.collection.query(
                query_embeddings=[all_docs["embeddings"][i]],
                n_results=10  # æŸ¥æ‰¾å‰10ä¸ªæœ€ç›¸ä¼¼çš„
            )
            
            # æ£€æŸ¥ç›¸ä¼¼åº¦ï¼ˆChromaDBè¿”å›è·ç¦»ï¼Œéœ€è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
            for j, distance in enumerate(results["distances"][0]):
                # ä½™å¼¦ç›¸ä¼¼åº¦ = 1 - ä½™å¼¦è·ç¦»
                similarity = 1 - distance
                
                if similarity > similarity_threshold:
                    duplicate_id = results["ids"][0][j]
                    if duplicate_id != all_docs["ids"][i]:
                        duplicates.add(duplicate_id)
        
        print(f"âœ“ å‘ç° {len(duplicates)} ä¸ªé‡å¤æ–‡æ¡£")
        
        # è¿”å›éé‡å¤æ–‡æ¡£ID
        unique_ids = [doc_id for doc_id in all_docs["ids"] if doc_id not in duplicates]
        return unique_ids


# ===============================================
# ç¬¬4æ­¥: èšç±»é‡‡æ ·æ¨¡å—
# ===============================================
class DiversitySampler:
    """
    å¤šæ ·æ€§é‡‡æ ·å™¨ - åŸºäºK-meansèšç±»
    
    ç­–ç•¥:
    1. K-meansèšç±»å°†æ•°æ®åˆ†ä¸ºKç±»
    2. ä»æ¯ç±»ä¸­æŒ‰æ¯”ä¾‹é‡‡æ ·
    3. ä¿è¯è¦†ç›–æ‰€æœ‰ä¸»é¢˜/æ¨¡å¼
    """
    
    def __init__(self, n_clusters: int = 100, random_state: int = 42):
        """
        åˆå§‹åŒ–èšç±»é‡‡æ ·å™¨
        
        å‚æ•°:
        - n_clusters: èšç±»æ•°é‡ï¼ˆå»ºè®®100-500ï¼‰
        - random_state: éšæœºç§å­
        """
        from sklearn.cluster import MiniBatchKMeans
        
        self.n_clusters = n_clusters
        self.kmeans = MiniBatchKMeans(
            n_clusters=n_clusters,
            random_state=random_state,
            batch_size=1000,  # Mini-batchåŠ é€Ÿ
            verbose=1
        )
    
    def fit_predict(self, embeddings: np.ndarray) -> np.ndarray:
        """
        èšç±»å¹¶é¢„æµ‹æ¯ä¸ªæ ·æœ¬çš„ç±»åˆ«
        
        å‚æ•°:
        - embeddings: (N, D) åµŒå…¥çŸ©é˜µ
        
        è¿”å›:
        - labels: (N,) ç±»åˆ«æ ‡ç­¾
        """
        print(f"æ‰§è¡ŒK-meansèšç±» (k={self.n_clusters})...")
        labels = self.kmeans.fit_predict(embeddings)
        print(f"âœ“ èšç±»å®Œæˆ")
        
        # ç»Ÿè®¡æ¯ç±»æ ·æœ¬æ•°
        unique, counts = np.unique(labels, return_counts=True)
        print(f"èšç±»åˆ†å¸ƒ: min={counts.min()}, max={counts.max()}, mean={counts.mean():.0f}")
        
        return labels
    
    def stratified_sample(
        self,
        texts: List[str],
        embeddings: np.ndarray,
        labels: np.ndarray,
        n_samples: int = 10000,
        strategy: str = "balanced",
        min_per_cluster: int = 5,
        max_per_cluster: int = None,
        frequencies: List[int] = None  # æ–°å¢ï¼šé¢‘ç‡ä¿¡æ¯ç”¨äºåŠ æƒé‡‡æ ·
    ) -> Tuple[List[str], np.ndarray, np.ndarray, Dict]:
        """
        æ”¹è¿›çš„åˆ†å±‚é‡‡æ · - ç¡®ä¿æ¯ä¸ªèšç±»éƒ½æœ‰ä»£è¡¨ï¼Œä¿è¯å¤šæ ·æ€§å’Œå®Œæ•´æ€§
        
        å‚æ•°:
        - texts: æ–‡æœ¬åˆ—è¡¨
        - embeddings: åµŒå…¥çŸ©é˜µ
        - labels: èšç±»æ ‡ç­¾
        - n_samples: ç›®æ ‡é‡‡æ ·æ•°é‡
        - strategy: é‡‡æ ·ç­–ç•¥
            - 'balanced': æ¯ç±»é‡‡æ ·ç›¸åŒæ•°é‡
            - 'proportional': æŒ‰ç±»åˆ«æ¯”ä¾‹é‡‡æ ·
            - 'hybrid': æ··åˆç­–ç•¥ï¼ˆä¿è¯æœ€å°å€¼+æŒ‰æ¯”ä¾‹ï¼‰
        - min_per_cluster: æ¯ä¸ªç°‡æœ€å°‘é‡‡æ ·æ•°é‡ï¼ˆä¿è¯å®Œæ•´æ€§ï¼‰
        - max_per_cluster: æ¯ä¸ªç°‡æœ€å¤šé‡‡æ ·æ•°é‡ï¼ˆé˜²æ­¢æŸäº›ç°‡è¿‡åº¦ä»£è¡¨ï¼‰
        - frequencies: æ–‡æœ¬é¢‘ç‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ã€‚å¦‚æœæä¾›ï¼Œå°†ç”¨äºåŠ æƒé‡‡æ ·ï¼Œé«˜é¢‘æ ·æœ¬æ›´å¯èƒ½è¢«é€‰ä¸­
        
        è¿”å›:
        - sampled_texts: é‡‡æ ·åçš„æ–‡æœ¬
        - sampled_embeddings: é‡‡æ ·åçš„åµŒå…¥
        - sampled_indices: é‡‡æ ·çš„åŸå§‹ç´¢å¼•
        - sampling_stats: é‡‡æ ·ç»Ÿè®¡ä¿¡æ¯
        """
        print(f"åˆ†å±‚é‡‡æ · (ç›®æ ‡: {n_samples}æ¡, ç­–ç•¥: {strategy}, æ¯ç°‡æœ€å°‘: {min_per_cluster})...")
        
        # ç»Ÿè®¡æ¯ä¸ªç°‡çš„æ ·æœ¬æ•°
        unique_labels, cluster_counts = np.unique(labels, return_counts=True)
        n_active_clusters = len(unique_labels)
        
        print(f"æ´»è·ƒç°‡æ•°: {n_active_clusters}/{self.n_clusters}")
        print(f"ç°‡å¤§å°åˆ†å¸ƒ: min={cluster_counts.min()}, max={cluster_counts.max()}, mean={cluster_counts.mean():.1f}")
        
        # é¢‘ç‡æ„ŸçŸ¥é‡‡æ ·
        use_frequency_weighting = frequencies is not None
        if use_frequency_weighting:
            frequencies_array = np.array(frequencies, dtype=float)
            # ä½¿ç”¨sqrtå¹³æ»‘ï¼Œé¿å…è¶…é«˜é¢‘æ ·æœ¬å æ®è¿‡å¤šæƒé‡
            freq_weights = np.sqrt(frequencies_array)
            print(f"âœ“ å¯ç”¨é¢‘ç‡æ„ŸçŸ¥é‡‡æ · (é«˜é¢‘æ ·æœ¬æ›´å¯èƒ½è¢«é€‰ä¸­)")
        
        # ===== ç¬¬ä¸€é˜¶æ®µï¼šç¡®ä¿æ¯ä¸ªç°‡è‡³å°‘é‡‡æ · min_per_cluster æ¡ =====
        sampled_indices = []
        cluster_sample_counts = {}  # è®°å½•æ¯ä¸ªç°‡é‡‡æ ·äº†å¤šå°‘æ¡
        
        print("é˜¶æ®µ1: ç¡®ä¿æ¯ä¸ªç°‡çš„æœ€å°é‡‡æ ·é‡...")
        for cluster_id in unique_labels:
            cluster_mask = (labels == cluster_id)
            cluster_indices = np.where(cluster_mask)[0]
            
            # ç¡®å®šè¯¥ç°‡çš„æœ€å°é‡‡æ ·æ•°
            n_min = min(min_per_cluster, len(cluster_indices))
            
            if n_min > 0:
                # å¦‚æœæœ‰é¢‘ç‡ä¿¡æ¯ï¼Œä½¿ç”¨åŠ æƒé‡‡æ ·
                if use_frequency_weighting:
                    cluster_weights = freq_weights[cluster_indices]
                    cluster_weights = cluster_weights / cluster_weights.sum()  # å½’ä¸€åŒ–
                    selected = np.random.choice(
                        cluster_indices,
                        size=n_min,
                        replace=False,
                        p=cluster_weights
                    )
                else:
                    selected = np.random.choice(
                        cluster_indices,
                        size=n_min,
                        replace=False
                    )
                sampled_indices.extend(selected)
                cluster_sample_counts[cluster_id] = n_min
        
        print(f"  å·²é‡‡æ ·: {len(sampled_indices)} æ¡ (æœ€å°ä¿è¯)")
        
        # ===== ç¬¬äºŒé˜¶æ®µï¼šæ ¹æ®ç­–ç•¥åˆ†é…å‰©ä½™é…é¢ =====
        remaining_quota = n_samples - len(sampled_indices)
        
        if remaining_quota > 0:
            print(f"é˜¶æ®µ2: åˆ†é…å‰©ä½™ {remaining_quota} æ¡é…é¢...")
            
            # è®¡ç®—æ¯ä¸ªç°‡è¿˜èƒ½é‡‡æ ·å¤šå°‘
            cluster_allocation = {}
            
            for cluster_id in unique_labels:
                cluster_mask = (labels == cluster_id)
                cluster_indices = np.where(cluster_mask)[0]
                cluster_size = len(cluster_indices)
                already_sampled = cluster_sample_counts.get(cluster_id, 0)
                available = cluster_size - already_sampled
                
                if available <= 0:
                    cluster_allocation[cluster_id] = 0
                    continue
                
                # æ ¹æ®ç­–ç•¥è®¡ç®—åˆ†é…é‡
                if strategy == "balanced":
                    # å¹³å‡åˆ†é…å‰©ä½™é…é¢
                    target = remaining_quota // n_active_clusters
                elif strategy == "proportional":
                    # æŒ‰ç°‡å¤§å°æ¯”ä¾‹åˆ†é…
                    target = int(remaining_quota * cluster_size / len(texts))
                else:  # hybrid
                    # æ··åˆï¼šä¿è¯å¹³å‡+æŒ‰æ¯”ä¾‹è°ƒæ•´
                    base_allocation = remaining_quota // n_active_clusters
                    proportion_bonus = int((cluster_size / len(texts)) * remaining_quota * 0.3)
                    target = base_allocation + proportion_bonus
                
                # é™åˆ¶åœ¨å¯ç”¨èŒƒå›´å†…
                target = min(target, available)
                
                # åº”ç”¨æœ€å¤§å€¼é™åˆ¶
                if max_per_cluster is not None:
                    target = min(target, max_per_cluster - already_sampled)
                
                cluster_allocation[cluster_id] = max(0, target)
            
            # æ ‡å‡†åŒ–åˆ†é…ï¼ˆç¡®ä¿æ€»å’Œä¸è¶…è¿‡å‰©ä½™é…é¢ï¼‰
            total_allocated = sum(cluster_allocation.values())
            if total_allocated > remaining_quota:
                scale_factor = remaining_quota / total_allocated
                cluster_allocation = {
                    cid: int(count * scale_factor) 
                    for cid, count in cluster_allocation.items()
                }
            
            # æ‰§è¡Œç¬¬äºŒé˜¶æ®µé‡‡æ ·
            for cluster_id, n_additional in cluster_allocation.items():
                if n_additional <= 0:
                    continue
                
                cluster_mask = (labels == cluster_id)
                cluster_indices = np.where(cluster_mask)[0]
                
                # æ’é™¤å·²é‡‡æ ·çš„
                available_indices = list(set(cluster_indices) - set(sampled_indices))
                
                if len(available_indices) > 0:
                    n_sample = min(n_additional, len(available_indices))
                    
                    # å¦‚æœæœ‰é¢‘ç‡ä¿¡æ¯ï¼Œä½¿ç”¨åŠ æƒé‡‡æ ·
                    if use_frequency_weighting:
                        available_weights = freq_weights[available_indices]
                        available_weights = available_weights / available_weights.sum()
                        selected = np.random.choice(
                            available_indices,
                            size=n_sample,
                            replace=False,
                            p=available_weights
                        )
                    else:
                        selected = np.random.choice(
                            available_indices,
                            size=n_sample,
                            replace=False
                        )
                    sampled_indices.extend(selected)
                    cluster_sample_counts[cluster_id] = cluster_sample_counts.get(cluster_id, 0) + n_sample
            
            print(f"  å·²é‡‡æ ·: {len(sampled_indices)} æ¡ (å«ç¬¬äºŒé˜¶æ®µ)")
        
        # ===== ç¬¬ä¸‰é˜¶æ®µï¼šå¦‚æœä»ä¸è¶³ç›®æ ‡ï¼Œä»å¤§ç°‡è¡¥å…… =====
        if len(sampled_indices) < n_samples:
            remaining = n_samples - len(sampled_indices)
            print(f"é˜¶æ®µ3: è¡¥å……å‰©ä½™ {remaining} æ¡...")
            
            # æŒ‰ç°‡å¤§å°æ’åºï¼Œä»å¤§åˆ°å°è¡¥å……
            sorted_clusters = sorted(
                unique_labels,
                key=lambda cid: np.sum(labels == cid),
                reverse=True
            )
            
            for cluster_id in sorted_clusters:
                if remaining <= 0:
                    break
                
                cluster_mask = (labels == cluster_id)
                cluster_indices = np.where(cluster_mask)[0]
                available_indices = list(set(cluster_indices) - set(sampled_indices))
                
                if len(available_indices) > 0:
                    n_sample = min(remaining, len(available_indices))
                    
                    # å¦‚æœæœ‰é¢‘ç‡ä¿¡æ¯ï¼Œä½¿ç”¨åŠ æƒé‡‡æ ·
                    if use_frequency_weighting:
                        available_weights = freq_weights[available_indices]
                        available_weights = available_weights / available_weights.sum()
                        selected = np.random.choice(
                            available_indices,
                            size=n_sample,
                            replace=False,
                            p=available_weights
                        )
                    else:
                        selected = np.random.choice(
                            available_indices,
                            size=n_sample,
                            replace=False
                        )
                    sampled_indices.extend(selected)
                    cluster_sample_counts[cluster_id] = cluster_sample_counts.get(cluster_id, 0) + n_sample
                    remaining -= n_sample
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        sampled_indices = np.array(sampled_indices[:n_samples])
        
        # ===== ç”Ÿæˆé‡‡æ ·ç»Ÿè®¡ =====
        sampling_stats = {
            'total_samples': len(sampled_indices),
            'n_active_clusters': n_active_clusters,
            'n_covered_clusters': len(cluster_sample_counts),
            'coverage_rate': len(cluster_sample_counts) / n_active_clusters,
            'samples_per_cluster': cluster_sample_counts,
            'min_samples_per_cluster': min(cluster_sample_counts.values()) if cluster_sample_counts else 0,
            'max_samples_per_cluster': max(cluster_sample_counts.values()) if cluster_sample_counts else 0,
            'mean_samples_per_cluster': np.mean(list(cluster_sample_counts.values())) if cluster_sample_counts else 0,
        }
        
        print(f"\nâœ“ é‡‡æ ·å®Œæˆ: {len(sampled_indices)} æ¡")
        print(f"  ç°‡è¦†ç›–ç‡: {sampling_stats['coverage_rate']:.1%} ({sampling_stats['n_covered_clusters']}/{n_active_clusters})")
        print(f"  æ¯ç°‡é‡‡æ ·: min={sampling_stats['min_samples_per_cluster']}, "
              f"max={sampling_stats['max_samples_per_cluster']}, "
              f"mean={sampling_stats['mean_samples_per_cluster']:.1f}")
        
        # æå–é‡‡æ ·ç»“æœ
        sampled_texts = [texts[i] for i in sampled_indices]
        sampled_embeddings = embeddings[sampled_indices]
        
        return sampled_texts, sampled_embeddings, sampled_indices, sampling_stats


# ===============================================
# ç¬¬5æ­¥: è´¨é‡è¯„åˆ†æ¨¡å—
# ===============================================
class QualityScorer:
    """
    è´¨é‡è¯„åˆ†å™¨ - å¤šç»´åº¦è¯„ä¼°æ–‡æœ¬è´¨é‡
    
    è¯„åˆ†ç»´åº¦:
    1. ä¿¡æ¯å¯†åº¦ - æ˜¯å¦åŒ…å«å®è´¨å†…å®¹
    2. è¯­æ³•å®Œæ•´æ€§ - æ˜¯å¦ä¸ºå®Œæ•´å¥å­
    3. é¢†åŸŸç›¸å…³æ€§ - æ˜¯å¦ä¸ç›®æ ‡é¢†åŸŸç›¸å…³
    4. å¯æ ‡æ³¨æ€§ - æ˜¯å¦é€‚åˆäººå·¥æ ‡æ³¨
    """
    
    def __init__(self, domain_keywords: List[str] = None):
        """
        åˆå§‹åŒ–è´¨é‡è¯„åˆ†å™¨
        
        å‚æ•°:
        - domain_keywords: é¢†åŸŸå…³é”®è¯åˆ—è¡¨ï¼ˆå¯¿é™©ç›¸å…³ï¼‰
        """
        if domain_keywords is None:
            # å¯¿é™©é¢†åŸŸå…³é”®è¯
            domain_keywords = [
                'å¯¿é™©', 'ç»ˆèº«å¯¿é™©', 'å®šæœŸå¯¿é™©', 'ä¿é™©', 'ä¿éšœ',
                'ç†èµ”', 'å—ç›Šäºº', 'ä¿é¢', 'ä¿è´¹', 'æŠ•ä¿',
                'ä¿å•', 'èº«æ•…', 'å…¨æ®‹', 'è´£ä»»', 'ç°é‡‘ä»·å€¼'
            ]
        
        self.domain_keywords = domain_keywords
    
    def score_information_density(self, text: str) -> float:
        """
        ä¿¡æ¯å¯†åº¦å¾—åˆ† (0-1)
        
        è§„åˆ™:
        - å­—æ•°é€‚ä¸­: 10-200å­— (1.0åˆ†)
        - åŒ…å«å®è¯: åè¯ã€åŠ¨è¯æ¯”ä¾‹
        """
        word_count = len(text)
        
        # å­—æ•°å¾—åˆ†
        if 10 <= word_count <= 200:
            length_score = 1.0
        elif word_count < 10:
            length_score = word_count / 10
        else:
            length_score = max(0.5, 1 - (word_count - 200) / 300)
        
        # å®è¯æ¯”ä¾‹ï¼ˆç®€å•å¯å‘å¼ï¼šä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ï¼‰
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        char_ratio = chinese_chars / max(len(text), 1)
        
        return (length_score + char_ratio) / 2
    
    def score_grammar_completeness(self, text: str) -> float:
        """
        è¯­æ³•å®Œæ•´æ€§å¾—åˆ† (0-1)
        
        è§„åˆ™:
        - æœ‰æ ‡ç‚¹ç¬¦å·: +0.3
        - ä»¥é—®å·ç»“å°¾: +0.3 (é—®å¥)
        - æ²¡æœ‰è¿ç»­ç‰¹æ®Šå­—ç¬¦: +0.4
        """
        score = 0.0
        
        # åŒ…å«æ ‡ç‚¹
        if re.search(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š]', text):
            score += 0.3
        
        # ä»¥é—®å·ç»“å°¾ï¼ˆé—®å¥æ›´é€‚åˆæ„å›¾è¯†åˆ«ï¼‰
        if text.endswith('ï¼Ÿ') or text.endswith('?'):
            score += 0.3
        
        # æ²¡æœ‰è¿ç»­ç‰¹æ®Šå­—ç¬¦
        if not re.search(r'[^\u4e00-\u9fffa-zA-Z0-9]{3,}', text):
            score += 0.4
        
        return min(score, 1.0)
    
    def score_domain_relevance(self, text: str) -> float:
        """
        é¢†åŸŸç›¸å…³æ€§å¾—åˆ† (0-1)
        
        è§„åˆ™:
        - åŒ…å«é¢†åŸŸå…³é”®è¯æ•°é‡
        """
        keyword_count = sum(1 for keyword in self.domain_keywords if keyword in text)
        
        # åŒ…å«1ä¸ªå…³é”®è¯: 0.5, åŒ…å«2ä¸ª: 0.75, åŒ…å«3+: 1.0
        if keyword_count == 0:
            return 0.0
        elif keyword_count == 1:
            return 0.5
        elif keyword_count == 2:
            return 0.75
        else:
            return 1.0
    
    def score_batch(self, texts: List[str]) -> np.ndarray:
        """
        æ‰¹é‡è¯„åˆ†
        
        è¿”å›:
        - scores: (N,) ç»¼åˆå¾—åˆ†æ•°ç»„
        """
        print("è®¡ç®—è´¨é‡å¾—åˆ†...")
        
        scores = []
        for text in tqdm(texts, desc="è´¨é‡è¯„åˆ†"):
            density = self.score_information_density(text)
            grammar = self.score_grammar_completeness(text)
            relevance = self.score_domain_relevance(text)
            
            # ç»¼åˆå¾—åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
            # é¢†åŸŸç›¸å…³æ€§æœ€é‡è¦ï¼Œå…¶æ¬¡æ˜¯ä¿¡æ¯å¯†åº¦å’Œè¯­æ³•
            final_score = (
                0.4 * relevance +
                0.3 * density +
                0.3 * grammar
            )
            
            scores.append(final_score)
        
        scores = np.array(scores)
        print(f"âœ“ å¾—åˆ†ç»Ÿè®¡: min={scores.min():.2f}, max={scores.max():.2f}, mean={scores.mean():.2f}")
        
        return scores


# ===============================================
# ä¸»Pipeline
# ===============================================
class SamplingPipeline:
    """
    å®Œæ•´çš„æ•°æ®é‡‡æ ·Pipeline
    
    æµç¨‹:
    1. åŠ è½½åŸå§‹æ•°æ®
    2. æ•°æ®æ¸…æ´—
    3. å‘é‡åµŒå…¥
    4. å‘é‡æ•°æ®åº“å­˜å‚¨
    5. å»é‡
    6. èšç±»é‡‡æ ·
    7. è´¨é‡è¯„åˆ†ä¸ç­›é€‰
    8. è¾“å‡ºå¾…æ ‡æ³¨æ•°æ®
    """
    
    def __init__(
        self,
        input_file: str,
        output_file: str = "data/sampled_for_annotation.csv",
        n_target_samples: int = 10000,
        n_clusters: int = 200,
        embedding_model: str = "moka-ai/m3e-base"
    ):
        """
        åˆå§‹åŒ–Pipeline
        
        å‚æ•°:
        - input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆ400ä¸‡æ¡logï¼‰
        - output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆ10000æ¡å¾…æ ‡æ³¨æ•°æ®ï¼‰
        - n_target_samples: ç›®æ ‡é‡‡æ ·æ•°é‡
        - n_clusters: èšç±»æ•°é‡
        - embedding_model: åµŒå…¥æ¨¡å‹åç§°
        """
        self.input_file = input_file
        self.output_file = output_file
        self.n_target_samples = n_target_samples
        self.n_clusters = n_clusters
        self.embedding_model = embedding_model
        
        # åˆå§‹åŒ–å„æ¨¡å—
        self.cleaner = DataCleaner()
        self.embedder = None  # å»¶è¿ŸåŠ è½½
        self.vector_db = None  # å»¶è¿ŸåŠ è½½
        self.sampler = None  # å»¶è¿ŸåŠ è½½
        self.scorer = QualityScorer()
    
    def load_data(self) -> pd.DataFrame:
        """
        åŠ è½½åŸå§‹æ•°æ®
        
        å‡è®¾æ•°æ®æ ¼å¼:
        - CSVæ–‡ä»¶ï¼ŒåŒ…å«'text'åˆ—ï¼ˆæˆ–å…¶ä»–åˆ—åï¼‰
        """
        print(f"åŠ è½½æ•°æ®: {self.input_file}")
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½
        if self.input_file.endswith('.csv'):
            # å°è¯•å¤šç§ç¼–ç å’Œæ–¹å¼åŠ è½½CSV
            encodings_to_try = ['utf-8', 'gbk', 'gb18030', 'gb2312', 'latin1', 'iso-8859-1']
            df = None
            
            # å…ˆå°è¯•ä¸åŒç¼–ç 
            for encoding in encodings_to_try:
                try:
                    print(f"å°è¯•ç¼–ç : {encoding}")
                    df = pd.read_csv(
                        self.input_file,
                        on_bad_lines='skip',  # è·³è¿‡é”™è¯¯è¡Œ
                        encoding=encoding,
                        engine='python'  # ä½¿ç”¨æ›´å®½å®¹çš„Pythonå¼•æ“
                    )
                    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆï¼ˆç¼–ç : {encoding}ï¼‰: {len(df)} æ¡")
                    break
                except UnicodeDecodeError as e:
                    print(f"  ç¼–ç  {encoding} å¤±è´¥: {str(e)[:80]}")
                    continue
                except Exception as e:
                    print(f"  ç¼–ç  {encoding} å‡ºé”™: {str(e)[:80]}")
                    continue
            
            # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•å¿½ç•¥é”™è¯¯
            if df is None:
                print(f"âš ï¸ æ‰€æœ‰æ ‡å‡†ç¼–ç å¤±è´¥ï¼Œå°è¯•å¿½ç•¥ç¼–ç é”™è¯¯...")
                try:
                    df = pd.read_csv(
                        self.input_file,
                        on_bad_lines='skip',
                        encoding='utf-8',
                        encoding_errors='ignore',  # å¿½ç•¥ç¼–ç é”™è¯¯
                        engine='python'
                    )
                    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆï¼ˆå¿½ç•¥ç¼–ç é”™è¯¯ï¼‰: {len(df)} æ¡")
                except Exception as e:
                    print(f"âš ï¸ å¿½ç•¥ç¼–ç é”™è¯¯ä¹Ÿå¤±è´¥: {str(e)[:100]}")
                    
                    # æœ€åå°è¯•å…¼å®¹æ—§ç‰ˆæœ¬
                    try:
                        df = pd.read_csv(
                            self.input_file,
                            error_bad_lines=False,
                            warn_bad_lines=False,
                            encoding='gbk',
                            engine='python'
                        )
                        print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆï¼ˆå…¼å®¹æ¨¡å¼+GBKï¼‰: {len(df)} æ¡")
                    except Exception as e2:
                        print(f"âš ï¸ å…¼å®¹æ¨¡å¼å¤±è´¥: {str(e2)[:100]}")
                        print(f"å°è¯•æœ€å®½å®¹æ¨¡å¼...")
                        
                        # æœ€åæ‰‹æ®µ: é€è¡Œè¯»å–
                        df = self._load_csv_tolerant(self.input_file)
                        print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆï¼ˆå®½å®¹æ¨¡å¼ï¼‰: {len(df)} æ¡")
                    
        elif self.input_file.endswith('.json'):
            df = pd.read_json(self.input_file, lines=True)
        elif self.input_file.endswith('.parquet'):
            df = pd.read_parquet(self.input_file)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {self.input_file}")
        
        print(f"åˆ—å: {df.columns.tolist()}")
        
        return df
    
    def _load_csv_tolerant(self, filepath: str) -> pd.DataFrame:
        """
        æœ€å®½å®¹çš„CSVåŠ è½½æ–¹å¼ï¼ˆé€è¡Œå¤„ç†ï¼‰
        
        å½“æ ‡å‡†æ–¹æ³•å¤±è´¥æ—¶ä½¿ç”¨
        """
        import csv
        
        print(f"ä½¿ç”¨é€è¡Œè¯»å–æ¨¡å¼ï¼ˆè¾ƒæ…¢ä½†æœ€ç¨³å®šï¼‰...")
        
        # å°è¯•å¤šç§ç¼–ç 
        encodings_to_try = ['utf-8', 'gbk', 'gb18030', 'gb2312', 'latin1', 'iso-8859-1']
        
        for encoding in encodings_to_try:
            rows = []
            error_count = 0
            
            try:
                print(f"  å°è¯•ç¼–ç : {encoding}")
                with open(filepath, 'r', encoding=encoding, errors='ignore') as f:
                    # å°è¯•æ£€æµ‹CSVæ ¼å¼
                    sample = f.read(1024)
                    f.seek(0)
                    
                    try:
                        dialect = csv.Sniffer().sniff(sample)
                    except:
                        dialect = csv.excel  # ä½¿ç”¨é»˜è®¤
                    
                    reader = csv.reader(f, dialect)
                    
                    # è¯»å–è¡¨å¤´
                    try:
                        headers = next(reader)
                    except StopIteration:
                        raise ValueError("æ–‡ä»¶ä¸ºç©º")
                    
                    # é€è¡Œè¯»å–
                    for line_num, row in enumerate(reader, start=2):
                        try:
                            if len(row) == len(headers):
                                rows.append(row)
                            else:
                                # åˆ—æ•°ä¸åŒ¹é…ï¼Œå°è¯•ä¿®æ­£
                                if len(row) < len(headers):
                                    row.extend([''] * (len(headers) - len(row)))
                                else:
                                    row = row[:len(headers)]
                                rows.append(row)
                        except Exception as e:
                            error_count += 1
                            if error_count <= 10:  # åªæ‰“å°å‰10ä¸ªé”™è¯¯
                                print(f"    è·³è¿‡è¡Œ {line_num}: {str(e)[:50]}")
                
                # å¦‚æœæˆåŠŸè¯»å–äº†æ•°æ®ï¼Œè·³å‡ºå¾ªç¯
                if len(rows) > 0:
                    print(f"  âœ“ ä½¿ç”¨ç¼–ç  {encoding} æˆåŠŸè¯»å–")
                    if error_count > 0:
                        print(f"  âš ï¸ è·³è¿‡äº† {error_count} ä¸ªé”™è¯¯è¡Œ")
                    
                    # æ„å»ºDataFrame
                    df = pd.DataFrame(rows, columns=headers)
                    return df
                    
            except Exception as e:
                print(f"  ç¼–ç  {encoding} å¤±è´¥: {str(e)[:80]}")
                continue
        
        # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥
        raise ValueError(f"æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç è¯»å–æ–‡ä»¶: {filepath}")
    
    def run(self):
        """è¿è¡Œå®Œæ•´Pipeline"""
        
        print("=" * 70)
        print("æ•°æ®é‡‡æ ·Pipelineå¯åŠ¨")
        print(f"è¾“å…¥: {self.input_file}")
        print(f"ç›®æ ‡: ä»400ä¸‡æ¡ä¸­ç­›é€‰ {self.n_target_samples} æ¡")
        print("=" * 70)
        
        # ========== æ­¥éª¤1: åŠ è½½æ•°æ® ==========
        df = self.load_data()
        
        # å‡è®¾æ–‡æœ¬åˆ—åä¸º'text'æˆ–'query'æˆ–'question'
        text_column = None
        for col in ['text', 'query', 'question', 'content', 'message']:
            if col in df.columns:
                text_column = col
                break
        
        if text_column is None:
            print(f"é”™è¯¯: æœªæ‰¾åˆ°æ–‡æœ¬åˆ—ï¼Œè¯·æŒ‡å®šåˆ—å")
            print(f"å¯ç”¨åˆ—: {df.columns.tolist()}")
            return
        
        print(f"ä½¿ç”¨æ–‡æœ¬åˆ—: '{text_column}'")
        texts = df[text_column].tolist()
        
        # ========== æ­¥éª¤2: æ•°æ®æ¸…æ´— ==========
        print("\n" + "=" * 70)
        print("æ­¥éª¤1: æ•°æ®æ¸…æ´—")
        print("=" * 70)
        
        cleaned_texts, valid_indices = self.cleaner.filter_batch(texts)
        
        print(f"âœ“ æ¸…æ´—å®Œæˆ: {len(cleaned_texts)} / {len(texts)} ({len(cleaned_texts)/len(texts)*100:.1f}%)")
        
        # å¦‚æœæ¸…æ´—åæ•°æ®é‡ä»ç„¶å¾ˆå¤§ï¼Œå¯ä»¥å…ˆéšæœºé‡‡æ ·åˆ°100ä¸‡
        if len(cleaned_texts) > 1_000_000:
            print(f"æ•°æ®é‡è¾ƒå¤§({len(cleaned_texts)}æ¡)ï¼Œå…ˆéšæœºé‡‡æ ·åˆ°100ä¸‡æ¡ä»¥åŠ é€Ÿå¤„ç†...")
            sample_indices = np.random.choice(
                len(cleaned_texts),
                size=1_000_000,
                replace=False
            )
            cleaned_texts = [cleaned_texts[i] for i in sample_indices]
            valid_indices = [valid_indices[i] for i in sample_indices]
        
        # ========== æ­¥éª¤3: æ–‡æœ¬å»é‡ï¼ˆä¿ç•™é¢‘ç‡ä¿¡æ¯ï¼‰==========
        print("\n" + "=" * 70)
        print("æ­¥éª¤2: å»é‡å¹¶ç»Ÿè®¡é¢‘ç‡")
        print("=" * 70)
        
        # ç»Ÿè®¡é¢‘ç‡
        from collections import Counter
        text_freq = Counter(cleaned_texts)
        
        # åˆ›å»ºDataFrame
        df_with_freq = pd.DataFrame({'text': cleaned_texts, 'original_index': valid_indices})
        
        # å»é‡ä½†ä¿ç•™é¢‘ç‡ä¿¡æ¯
        unique_df = df_with_freq.drop_duplicates(subset=['text'], keep='first').copy()
        unique_df['frequency'] = unique_df['text'].map(text_freq)
        
        # é¢‘ç‡ç»Ÿè®¡
        freq_values = unique_df['frequency'].values
        print(f"âœ“ å»é‡å®Œæˆ: {len(unique_df)} æ¡å”¯ä¸€æ–‡æœ¬")
        print(f"  é¢‘ç‡åˆ†å¸ƒ: min={freq_values.min()}, max={freq_values.max()}, mean={freq_values.mean():.1f}, median={np.median(freq_values):.1f}")
        print(f"  é«˜é¢‘æ ·æœ¬(>=10æ¬¡): {np.sum(freq_values >= 10)} æ¡ ({np.sum(freq_values >= 10)/len(freq_values):.1%})")
        print(f"  ä¸­é¢‘æ ·æœ¬(2-9æ¬¡): {np.sum((freq_values >= 2) & (freq_values < 10))} æ¡")
        print(f"  ä½é¢‘æ ·æœ¬(1æ¬¡): {np.sum(freq_values == 1)} æ¡ ({np.sum(freq_values == 1)/len(freq_values):.1%})")
        print(f"  æ€»é¢‘ç‡è¦†ç›–: {freq_values.sum():,} æ¬¡åŸå§‹æŸ¥è¯¢")
        
        # å»é‡ç‡åˆ†æ
        dedup_rate = len(unique_df) / len(df_with_freq)
        print(f"  å»é‡ç‡: {dedup_rate:.1%}")
        
        if dedup_rate < 0.7:
            print(f"  ğŸ’¡ å»é‡ç‡è¾ƒä½ï¼Œè¯´æ˜æœ‰å¤§é‡é‡å¤æŸ¥è¯¢ï¼Œå»ºè®®ä½¿ç”¨é¢‘ç‡æ„ŸçŸ¥é‡‡æ ·")
        
        cleaned_texts = unique_df['text'].tolist()
        valid_indices = unique_df['original_index'].tolist()
        text_frequencies = unique_df['frequency'].tolist()
        
        # ========== æ­¥éª¤4: å‘é‡åµŒå…¥ ==========
        print("\n" + "=" * 70)
        print("æ­¥éª¤3: ç”Ÿæˆæ–‡æœ¬åµŒå…¥")
        print("=" * 70)
        
        self.embedder = TextEmbedder(model_name=self.embedding_model)
        embeddings = self.embedder.embed_batch(cleaned_texts, batch_size=64)
        
        print(f"âœ“ åµŒå…¥ç”Ÿæˆå®Œæˆ: shape={embeddings.shape}")
        
        # ========== æ­¥éª¤5: å‘é‡æ•°æ®åº“å­˜å‚¨ï¼ˆå¯é€‰ï¼Œç”¨äºè¯­ä¹‰å»é‡ï¼‰==========
        # æ³¨æ„ï¼šå¯¹äº100ä¸‡+æ•°æ®ï¼Œå‘é‡æ•°æ®åº“å»é‡ä¼šå¾ˆæ…¢
        # è¿™é‡Œè·³è¿‡ï¼Œæˆ–è€…åªå¯¹æœ€ç»ˆé‡‡æ ·åçš„æ•°æ®åšè¯­ä¹‰å»é‡
        
        # ========== æ­¥éª¤6: K-meansèšç±»é‡‡æ · ==========
        print("\n" + "=" * 70)
        print("æ­¥éª¤4: K-meansèšç±» + å¤šæ ·æ€§é‡‡æ ·")
        print("=" * 70)
        
        # å…ˆé‡‡æ ·åˆ°3å€ç›®æ ‡æ•°é‡ï¼Œç•™ç»™è´¨é‡è¯„åˆ†ç­›é€‰
        # å¯¹äº98% F1ç›®æ ‡ï¼Œå»ºè®®æœ€ç»ˆæ•°æ®é‡15000-20000æ¡
        n_intermediate_samples = self.n_target_samples * 3
        
        self.sampler = DiversitySampler(n_clusters=self.n_clusters)
        labels = self.sampler.fit_predict(embeddings)
        
        # ä½¿ç”¨æ”¹è¿›çš„åˆ†å±‚é‡‡æ ·ï¼ˆç¡®ä¿æ¯ä¸ªç°‡éƒ½æœ‰ä»£è¡¨ï¼Œè€ƒè™‘é¢‘ç‡æƒé‡ï¼‰
        sampled_texts, sampled_embeddings, sampled_indices, sampling_stats = self.sampler.stratified_sample(
            texts=cleaned_texts,
            embeddings=embeddings,
            labels=labels,
            n_samples=n_intermediate_samples,
            strategy="hybrid",  # æ··åˆç­–ç•¥ï¼šä¿è¯å¤šæ ·æ€§+æŒ‰æ¯”ä¾‹
            min_per_cluster=5,  # æ¯ç°‡è‡³å°‘5æ¡
            max_per_cluster=n_intermediate_samples // self.n_clusters * 3,  # é˜²æ­¢å•ç°‡è¿‡å¤š
            frequencies=text_frequencies  # æ–°å¢ï¼šä¼ å…¥é¢‘ç‡ä¿¡æ¯ç”¨äºåŠ æƒé‡‡æ ·
        )
        
        # æå–é‡‡æ ·æ•°æ®çš„é¢‘ç‡
        sampled_frequencies = [text_frequencies[i] for i in sampled_indices]
        
        # æ£€æŸ¥ç°‡è¦†ç›–ç‡
        if sampling_stats['coverage_rate'] < 0.95:
            print(f"\nâš ï¸ è­¦å‘Š: ç°‡è¦†ç›–ç‡ä»… {sampling_stats['coverage_rate']:.1%}ï¼Œå¯èƒ½å½±å“å¤šæ ·æ€§")
            print(f"å»ºè®®: å¢åŠ ç›®æ ‡é‡‡æ ·æ•°é‡æˆ–å‡å°‘èšç±»æ•°")
        else:
            print(f"âœ“ ç°‡è¦†ç›–ç‡ä¼˜ç§€: {sampling_stats['coverage_rate']:.1%}")
        
        # ========== æ­¥éª¤7: è´¨é‡è¯„åˆ†ä¸ç­›é€‰ï¼ˆä¿æŒç°‡å¹³è¡¡ï¼‰==========
        print("\n" + "=" * 70)
        print("æ­¥éª¤5: è´¨é‡è¯„åˆ†ä¸ç°‡å¹³è¡¡ç­›é€‰")
        print("=" * 70)
        
        scores = self.scorer.score_batch(sampled_texts)
        
        # è·å–é‡‡æ ·æ•°æ®çš„ç°‡æ ‡ç­¾
        sampled_labels = labels[sampled_indices]
        
        # ç°‡å¹³è¡¡ç­›é€‰ï¼šç¡®ä¿æœ€ç»ˆæ•°æ®ä»ä¿æŒç°‡å¤šæ ·æ€§
        print("æ‰§è¡Œç°‡å¹³è¡¡ç­›é€‰ï¼ˆä¿è¯è´¨é‡çš„åŒæ—¶ç»´æŒå¤šæ ·æ€§ï¼‰...")
        
        final_indices = []
        cluster_final_counts = {}
        
        # è®¡ç®—æ¯ä¸ªç°‡çš„ç›®æ ‡æ•°é‡ï¼ˆæŒ‰é‡‡æ ·æ¯”ä¾‹ï¼‰
        unique_sampled_labels = np.unique(sampled_labels)
        samples_per_cluster_target = {}
        
        for cluster_id in unique_sampled_labels:
            cluster_count_in_sample = np.sum(sampled_labels == cluster_id)
            # æŒ‰æ¯”ä¾‹åˆ†é…æœ€ç»ˆæ•°é‡ï¼Œä½†è‡³å°‘ä¿ç•™3æ¡
            target = max(3, int(self.n_target_samples * cluster_count_in_sample / len(sampled_labels)))
            samples_per_cluster_target[cluster_id] = target
        
        # æ ‡å‡†åŒ–ï¼ˆç¡®ä¿æ€»å’Œä¸è¶…è¿‡ç›®æ ‡ï¼‰
        total_target = sum(samples_per_cluster_target.values())
        if total_target > self.n_target_samples:
            scale = self.n_target_samples / total_target
            samples_per_cluster_target = {
                cid: max(3, int(count * scale))
                for cid, count in samples_per_cluster_target.items()
            }
        
        # ä»æ¯ä¸ªç°‡ä¸­é€‰æ‹©æœ€é«˜è´¨é‡çš„æ ·æœ¬
        for cluster_id in unique_sampled_labels:
            cluster_mask = (sampled_labels == cluster_id)
            cluster_sample_indices = np.where(cluster_mask)[0]
            cluster_scores = scores[cluster_sample_indices]
            
            # æŒ‰è´¨é‡æ’åº
            sorted_cluster_indices = cluster_sample_indices[np.argsort(cluster_scores)[::-1]]
            
            # å–è¯¥ç°‡çš„ç›®æ ‡æ•°é‡
            n_take = min(samples_per_cluster_target[cluster_id], len(sorted_cluster_indices))
            selected = sorted_cluster_indices[:n_take]
            
            final_indices.extend(selected)
            cluster_final_counts[cluster_id] = n_take
        
        # å¦‚æœæ•°é‡ä¸è¶³ï¼Œä»é«˜è´¨é‡æ ·æœ¬ä¸­è¡¥å……
        if len(final_indices) < self.n_target_samples:
            remaining = self.n_target_samples - len(final_indices)
            
            # æ‰¾åˆ°æœªè¢«é€‰ä¸­çš„æ ·æœ¬
            all_indices = set(range(len(sampled_texts)))
            selected_set = set(final_indices)
            remaining_indices = list(all_indices - selected_set)
            
            # æŒ‰è´¨é‡æ’åºè¡¥å……
            remaining_scores = scores[remaining_indices]
            top_remaining = [remaining_indices[i] for i in np.argsort(remaining_scores)[::-1][:remaining]]
            final_indices.extend(top_remaining)
        
        # æˆªæ–­åˆ°ç›®æ ‡æ•°é‡
        final_indices = final_indices[:self.n_target_samples]
        
        final_texts = [sampled_texts[i] for i in final_indices]
        final_scores = scores[final_indices]
        final_labels = sampled_labels[final_indices]
        final_original_indices = [valid_indices[sampled_indices[i]] for i in final_indices]
        final_frequencies = [sampled_frequencies[i] for i in final_indices]
        
        # éªŒè¯æœ€ç»ˆç°‡è¦†ç›–
        final_unique_clusters = len(np.unique(final_labels))
        final_coverage_rate = final_unique_clusters / len(unique_sampled_labels)
        
        print(f"âœ“ æœ€ç»ˆç­›é€‰: {len(final_texts)} æ¡")
        print(f"  è´¨é‡å¾—åˆ†: {final_scores.min():.2f} - {final_scores.max():.2f} (mean: {final_scores.mean():.2f})")
        print(f"  æœ€ç»ˆç°‡è¦†ç›–: {final_unique_clusters}/{len(unique_sampled_labels)} = {final_coverage_rate:.1%}")
        
        if final_coverage_rate < 0.90:
            print(f"âš ï¸ è­¦å‘Š: è´¨é‡ç­›é€‰åç°‡è¦†ç›–ç‡ä¸‹é™åˆ° {final_coverage_rate:.1%}")
            print(f"å»ºè®®: é™ä½è´¨é‡é˜ˆå€¼æˆ–å¢åŠ ä¸­é—´é‡‡æ ·æ•°é‡")
        
        # ========== æ­¥éª¤8: è¾“å‡ºå¾…æ ‡æ³¨æ•°æ® ==========
        print("\n" + "=" * 70)
        print("æ­¥éª¤6: ä¿å­˜å¾…æ ‡æ³¨æ•°æ®")
        print("=" * 70)
        
        output_df = pd.DataFrame({
            'text': final_texts,
            'frequency': final_frequencies,  # æ–°å¢ï¼šä¿ç•™é¢‘ç‡ä¿¡æ¯
            'quality_score': final_scores,
            'cluster_id': final_labels,
            'original_index': final_original_indices,
            'importance': ['é«˜é¢‘' if f >= 10 else 'ä¸­é¢‘' if f >= 2 else 'ä½é¢‘' for f in final_frequencies],  # æ–°å¢ï¼šé‡è¦æ€§æ ‡ç­¾
            'label': '',  # ç©ºç™½åˆ—ï¼Œç­‰å¾…äººå·¥æ ‡æ³¨
        })
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(self.output_file).parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜
        output_df.to_csv(self.output_file, index=False, encoding='utf-8-sig')
        print(f"âœ“ æ•°æ®å·²ä¿å­˜åˆ°: {self.output_file}")
        
        # ========== ä¿å­˜è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ ==========
        # è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡
        cluster_distribution = {}
        for cluster_id in np.unique(final_labels):
            count = np.sum(final_labels == cluster_id)
            cluster_distribution[int(cluster_id)] = int(count)
        
        # Shannonç†µï¼ˆè¡¡é‡ç°‡åˆ†å¸ƒçš„å‡åŒ€æ€§ï¼‰
        cluster_counts = np.array(list(cluster_distribution.values()))
        cluster_probs = cluster_counts / cluster_counts.sum()
        shannon_entropy = -np.sum(cluster_probs * np.log(cluster_probs + 1e-10))
        max_entropy = np.log(len(cluster_counts))
        normalized_entropy = shannon_entropy / max_entropy if max_entropy > 0 else 0
        
        stats = {
            # æ•°æ®æµç»Ÿè®¡
            'pipeline_stats': {
                'total_input': len(texts),
                'after_cleaning': len(cleaned_texts),
                'after_dedup': len(cleaned_texts),
                'after_clustering_sample': len(sampled_texts),
                'final_output': len(final_texts),
                'retention_rate': len(final_texts) / len(texts),
            },
            
            # èšç±»ç»Ÿè®¡
            'clustering_stats': {
                'n_clusters': self.n_clusters,
                'n_active_clusters': int(sampling_stats['n_active_clusters']),
                'n_covered_clusters_initial': int(sampling_stats['n_covered_clusters']),
                'n_covered_clusters_final': int(final_unique_clusters),
                'coverage_rate_initial': float(sampling_stats['coverage_rate']),
                'coverage_rate_final': float(final_coverage_rate),
            },
            
            # è´¨é‡ç»Ÿè®¡
            'quality_stats': {
                'score_min': float(final_scores.min()),
                'score_max': float(final_scores.max()),
                'score_mean': float(final_scores.mean()),
                'score_std': float(final_scores.std()),
                'score_median': float(np.median(final_scores)),
                'score_q25': float(np.percentile(final_scores, 25)),
                'score_q75': float(np.percentile(final_scores, 75)),
            },
            
            # å¤šæ ·æ€§ç»Ÿè®¡
            'diversity_stats': {
                'cluster_distribution': cluster_distribution,
                'samples_per_cluster_min': int(cluster_counts.min()),
                'samples_per_cluster_max': int(cluster_counts.max()),
                'samples_per_cluster_mean': float(cluster_counts.mean()),
                'samples_per_cluster_std': float(cluster_counts.std()),
                'shannon_entropy': float(shannon_entropy),
                'normalized_entropy': float(normalized_entropy),
                'entropy_interpretation': 'æ¥è¿‘1è¡¨ç¤ºåˆ†å¸ƒå‡åŒ€ï¼Œæ¥è¿‘0è¡¨ç¤ºåˆ†å¸ƒé›†ä¸­'
            },
            
            # å»ºè®®
            'recommendations': self._generate_recommendations(
                final_coverage_rate=final_coverage_rate,
                normalized_entropy=normalized_entropy,
                mean_quality=final_scores.mean(),
                n_samples=len(final_texts)
            )
        }
        
        stats_file = self.output_file.replace('.csv', '_stats.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, indent=2, fp=f, ensure_ascii=False)
        
        print(f"âœ“ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
        
        # ========== æ‰“å°æ€»ç»“æŠ¥å‘Š ==========
        print("\n" + "=" * 70)
        print("æ•°æ®è´¨é‡ä¸å¤šæ ·æ€§æŠ¥å‘Š")
        print("=" * 70)
        print(f"\nğŸ“Š æ•°æ®è§„æ¨¡:")
        print(f"  åŸå§‹æ•°æ®: {len(texts):,} æ¡")
        print(f"  æ¸…æ´—å: {len(cleaned_texts):,} æ¡")
        print(f"  æœ€ç»ˆè¾“å‡º: {len(final_texts):,} æ¡")
        
        print(f"\nğŸ¯ ç°‡è¦†ç›–ç‡:")
        print(f"  åˆå§‹é‡‡æ ·: {sampling_stats['coverage_rate']:.1%} ({sampling_stats['n_covered_clusters']}/{sampling_stats['n_active_clusters']})")
        print(f"  æœ€ç»ˆæ•°æ®: {final_coverage_rate:.1%} ({final_unique_clusters}/{len(unique_sampled_labels)})")
        
        print(f"\nâ­ è´¨é‡è¯„åˆ†:")
        print(f"  å¹³å‡åˆ†: {final_scores.mean():.3f}")
        print(f"  èŒƒå›´: {final_scores.min():.3f} - {final_scores.max():.3f}")
        
        print(f"\nğŸ”¥ é¢‘ç‡åˆ†å¸ƒ (åŸå§‹æŸ¥è¯¢é‡è¦æ€§):")
        final_freq_array = np.array(final_frequencies)
        total_freq = final_freq_array.sum()
        print(f"  é‡‡æ ·è¦†ç›–æ€»é¢‘ç‡: {total_freq:,} æ¬¡åŸå§‹æŸ¥è¯¢")
        print(f"  å¹³å‡é¢‘ç‡: {final_freq_array.mean():.1f}")
        print(f"  é«˜é¢‘æ ·æœ¬(>=10æ¬¡): {np.sum(final_freq_array >= 10)} æ¡ ({np.sum(final_freq_array >= 10)/len(final_freq_array):.1%})")
        print(f"  ä¸­é¢‘æ ·æœ¬(2-9æ¬¡): {np.sum((final_freq_array >= 2) & (final_freq_array < 10))} æ¡")
        print(f"  ä½é¢‘æ ·æœ¬(1æ¬¡): {np.sum(final_freq_array == 1)} æ¡ ({np.sum(final_freq_array == 1)/len(final_freq_array):.1%})")
        
        print(f"\nğŸŒˆ å¤šæ ·æ€§æŒ‡æ ‡:")
        print(f"  æ ‡å‡†åŒ–ç†µ: {normalized_entropy:.3f} (1.0ä¸ºæœ€ä½³)")
        print(f"  æ¯ç°‡æ ·æœ¬: {cluster_counts.min()}-{cluster_counts.max()} (å¹³å‡: {cluster_counts.mean():.1f})")
        
        print(f"\nğŸ’¡ å»ºè®®:")
        for rec in stats['recommendations']:
            print(f"  â€¢ {rec}")
        
        # ========== å®Œæˆ ==========
        print("\n" + "=" * 70)
        print("âœ“ Pipelineå®Œæˆï¼")
        print("=" * 70)
        print(f"\nğŸ“ ä¸‹ä¸€æ­¥:")
        print(f"1. æ‰“å¼€æ–‡ä»¶: {self.output_file}")
        print(f"2. å¯¹ 'label' åˆ—è¿›è¡Œäººå·¥æ ‡æ³¨ï¼ˆå¯¿é™©ç›¸å…³ / æ‹’è¯†ï¼‰")
        print(f"3. æ ‡æ³¨å®Œæˆåï¼Œä½¿ç”¨æ ‡æ³¨æ•°æ®è®­ç»ƒæ¨¡å‹")
        print(f"4. æŸ¥çœ‹è¯¦ç»†ç»Ÿè®¡: {stats_file}")
        print("=" * 70)
    
    def _generate_recommendations(
        self,
        final_coverage_rate: float,
        normalized_entropy: float,
        mean_quality: float,
        n_samples: int
    ) -> List[str]:
        """
        åŸºäºç»Ÿè®¡æŒ‡æ ‡ç”Ÿæˆå»ºè®®
        
        ç›®æ ‡: 98% F1 for 1.7B-8B LLMå¾®è°ƒ
        """
        recommendations = []
        
        # ç°‡è¦†ç›–ç‡å»ºè®®
        if final_coverage_rate < 0.90:
            recommendations.append(
                f"âš ï¸ ç°‡è¦†ç›–ç‡è¾ƒä½({final_coverage_rate:.1%})ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚"
                f"å»ºè®®å¢åŠ é‡‡æ ·æ•°é‡æˆ–é™ä½è´¨é‡é˜ˆå€¼ã€‚"
            )
        elif final_coverage_rate >= 0.95:
            recommendations.append(
                f"âœ“ ç°‡è¦†ç›–ç‡ä¼˜ç§€({final_coverage_rate:.1%})ï¼Œæ•°æ®å¤šæ ·æ€§è‰¯å¥½ã€‚"
            )
        
        # å¤šæ ·æ€§å»ºè®®
        if normalized_entropy < 0.7:
            recommendations.append(
                f"âš ï¸ æ•°æ®åˆ†å¸ƒé›†ä¸­(ç†µ={normalized_entropy:.2f})ï¼ŒæŸäº›ç°‡å¯èƒ½è¿‡åº¦ä»£è¡¨ã€‚"
                f"å»ºè®®ä½¿ç”¨'balanced'ç­–ç•¥æˆ–å¢åŠ min_per_clusterã€‚"
            )
        elif normalized_entropy >= 0.85:
            recommendations.append(
                f"âœ“ æ•°æ®åˆ†å¸ƒå‡åŒ€(ç†µ={normalized_entropy:.2f})ï¼Œå¤šæ ·æ€§ä¼˜ç§€ã€‚"
            )
        
        # è´¨é‡å»ºè®®
        if mean_quality < 0.4:
            recommendations.append(
                f"âš ï¸ å¹³å‡è´¨é‡è¾ƒä½({mean_quality:.2f})ï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆæœã€‚"
                f"å»ºè®®åŠ å¼ºæ•°æ®æ¸…æ´—æˆ–è°ƒæ•´è´¨é‡è¯„åˆ†æƒé‡ã€‚"
            )
        elif mean_quality >= 0.6:
            recommendations.append(
                f"âœ“ å¹³å‡è´¨é‡è‰¯å¥½({mean_quality:.2f})ã€‚"
            )
        
        # æ•°æ®é‡å»ºè®®ï¼ˆé’ˆå¯¹98% F1ç›®æ ‡ï¼‰
        if n_samples < 10000:
            recommendations.append(
                f"âš ï¸ æ•°æ®é‡è¾ƒå°‘({n_samples}æ¡)ï¼Œå¯¹äº1.7B-8Bæ¨¡å‹å¯èƒ½ä¸è¶³ã€‚"
                f"å»ºè®®å¢åŠ åˆ°15000-20000æ¡ä»¥è¾¾åˆ°98% F1ç›®æ ‡ã€‚"
            )
        elif n_samples >= 15000:
            recommendations.append(
                f"âœ“ æ•°æ®é‡å……è¶³({n_samples}æ¡)ï¼Œé€‚åˆ1.7B-8Bæ¨¡å‹å¾®è°ƒã€‚"
            )
        else:
            recommendations.append(
                f"æ•°æ®é‡({n_samples}æ¡)é€‚ä¸­ï¼Œå¯è€ƒè™‘å¢åŠ åˆ°15000-20000æ¡ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚"
            )
        
        # æ€»ä½“å»ºè®®
        if final_coverage_rate >= 0.95 and normalized_entropy >= 0.8 and mean_quality >= 0.5:
            recommendations.append(
                "ğŸ‰ æ•°æ®è´¨é‡ä¸å¤šæ ·æ€§å‡è¾¾æ ‡ï¼Œå¯ä»¥å¼€å§‹æ ‡æ³¨å’Œè®­ç»ƒï¼"
            )
        
        # é’ˆå¯¹98% F1çš„å…·ä½“å»ºè®®
        recommendations.append(
            "ğŸ’¡ è¾¾åˆ°98% F1çš„å…³é”®å› ç´ ï¼š"
        )
        recommendations.append(
            "  1. ç¡®ä¿æ¯ä¸ªæ„å›¾ç±»åˆ«è‡³å°‘æœ‰50-100æ¡æ ‡æ³¨æ ·æœ¬"
        )
        recommendations.append(
            "  2. å¯¹è¾¹ç•Œcaseè¿›è¡Œé‡ç‚¹æ ‡æ³¨ï¼ˆæ¨¡ç³Šã€æ­§ä¹‰æ ·æœ¬ï¼‰"
        )
        recommendations.append(
            "  3. ä½¿ç”¨ä¸»åŠ¨å­¦ä¹ ï¼šå…ˆè®­ç»ƒï¼Œæ‰¾å‡ºä½ç½®ä¿¡åº¦æ ·æœ¬ï¼Œè¡¥å……æ ‡æ³¨"
        )
        recommendations.append(
            "  4. è€ƒè™‘æ•°æ®å¢å¼ºï¼šåŒä¹‰æ›¿æ¢ã€å›è¯‘ç­‰"
        )
        
        return recommendations


# ===============================================
# å‘½ä»¤è¡Œå…¥å£
# ===============================================
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="æ•°æ®é‡‡æ ·Pipeline - ç¡®ä¿å¤šæ ·æ€§å’Œå®Œæ•´æ€§ï¼Œè¾¾åˆ°98% F1å¾®è°ƒç›®æ ‡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºç¡€ç”¨æ³•ï¼ˆ10000æ¡æ ·æœ¬ï¼‰
  python data_sampling_pipeline.py --input data/logs.csv
  
  # é’ˆå¯¹98% F1ç›®æ ‡ï¼ˆæ¨è15000-20000æ¡ï¼‰
  python data_sampling_pipeline.py --input data/logs.csv --n_samples 15000 --n_clusters 300
  
  # ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹
  python data_sampling_pipeline.py --input data/logs.csv --embedding_model ./models/m3e-base
  
  # ç¡®ä¿æ¯ä¸ªç°‡è‡³å°‘10æ¡æ ·æœ¬
  python data_sampling_pipeline.py --input data/logs.csv --n_samples 20000 --n_clusters 200 --min_per_cluster 10
        """
    )
    
    # åŸºç¡€å‚æ•°
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆ400ä¸‡æ¡logæ•°æ®ï¼Œæ”¯æŒCSV/JSON/Parquetï¼‰"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="data/sampled_for_annotation.csv",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: data/sampled_for_annotation.csv)"
    )
    
    # é‡‡æ ·å‚æ•°
    parser.add_argument(
        "--n_samples",
        type=int,
        default=10000,
        help="ç›®æ ‡é‡‡æ ·æ•°é‡ (å»ºè®®15000-20000ä»¥è¾¾åˆ°98%% F1) (é»˜è®¤: 10000)"
    )
    parser.add_argument(
        "--n_clusters",
        type=int,
        default=200,
        help="K-meansèšç±»æ•°é‡ (å½±å“å¤šæ ·æ€§ï¼Œå»ºè®®200-500) (é»˜è®¤: 200)"
    )
    
    # å¤šæ ·æ€§å‚æ•°
    parser.add_argument(
        "--min_per_cluster",
        type=int,
        default=5,
        help="æ¯ä¸ªç°‡æœ€å°‘é‡‡æ ·æ•°é‡ï¼Œç¡®ä¿å®Œæ•´æ€§ (é»˜è®¤: 5)"
    )
    parser.add_argument(
        "--sampling_strategy",
        type=str,
        default="hybrid",
        choices=["balanced", "proportional", "hybrid"],
        help="é‡‡æ ·ç­–ç•¥: balanced(å‡è¡¡), proportional(æŒ‰æ¯”ä¾‹), hybrid(æ··åˆ) (é»˜è®¤: hybrid)"
    )
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument(
        "--embedding_model",
        type=str,
        default="moka-ai/m3e-base",
        help="åµŒå…¥æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„ (é»˜è®¤: moka-ai/m3e-base)"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        choices=["cuda", "cpu"],
        help="è¿è¡Œè®¾å¤‡ (é»˜è®¤: cudaï¼Œè‡ªåŠ¨é™çº§åˆ°cpu)"
    )
    
    args = parser.parse_args()
    
    print(f"""
{'='*70}
æ•°æ®é‡‡æ ·Pipelineé…ç½®
{'='*70}
è¾“å…¥æ–‡ä»¶: {args.input}
è¾“å‡ºæ–‡ä»¶: {args.output}
ç›®æ ‡æ ·æœ¬æ•°: {args.n_samples}
èšç±»æ•°é‡: {args.n_clusters}
æ¯ç°‡æœ€å°‘: {args.min_per_cluster}
é‡‡æ ·ç­–ç•¥: {args.sampling_strategy}
åµŒå…¥æ¨¡å‹: {args.embedding_model}
{'='*70}
""")
    
    # è¿è¡ŒPipeline
    pipeline = SamplingPipeline(
        input_file=args.input,
        output_file=args.output,
        n_target_samples=args.n_samples,
        n_clusters=args.n_clusters,
        embedding_model=args.embedding_model
    )
    
    pipeline.run()


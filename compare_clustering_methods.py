"""
èšç±»æ–¹æ³•å¯¹æ¯”å®éªŒè„šæœ¬

ç”¨é€”ï¼šå¿«é€Ÿå¯¹æ¯”K-means, Hybrid, HDBSCANç­‰æ–¹æ³•åœ¨æ‚¨æ•°æ®ä¸Šçš„æ•ˆæœ

ä½¿ç”¨æ–¹æ³•ï¼š
    python compare_clustering_methods.py --input data/cleaned_460k.csv --n_samples 5000

è¾“å‡ºï¼š
    - å„æ–¹æ³•çš„ç°‡è¦†ç›–ç‡ã€æ ‡å‡†åŒ–ç†µã€è½®å»“ç³»æ•°
    - å¯è§†åŒ–å¯¹æ¯”å›¾è¡¨
    - æ¨èä½¿ç”¨å“ªç§æ–¹æ³•
"""

import pandas as pd
import numpy as np
import argparse
import time
from pathlib import Path
from typing import Dict, List
import json

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
import sys
sys.path.append(str(Path(__file__).parent))


def load_and_prepare_data(
    input_file: str,
    n_samples: int = 5000,
    embedding_model: str = "moka-ai/m3e-base"
) -> tuple:
    """
    åŠ è½½å¹¶å‡†å¤‡æµ‹è¯•æ•°æ®
    
    è¿”å›:
    - texts: æ–‡æœ¬åˆ—è¡¨
    - embeddings: embeddingçŸ©é˜µ
    """
    print(f"åŠ è½½æ•°æ®: {input_file}")
    
    # åŠ è½½æ•°æ®
    if input_file.endswith('.csv'):
        df = pd.read_csv(input_file, nrows=n_samples)
    elif input_file.endswith('.json'):
        df = pd.read_json(input_file, lines=True, nrows=n_samples)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {input_file}")
    
    # æ‰¾åˆ°æ–‡æœ¬åˆ—
    text_column = None
    for col in ['text', 'query', 'question', 'content', 'message']:
        if col in df.columns:
            text_column = col
            break
    
    if text_column is None:
        raise ValueError(f"æœªæ‰¾åˆ°æ–‡æœ¬åˆ—ï¼Œå¯ç”¨åˆ—: {df.columns.tolist()}")
    
    texts = df[text_column].tolist()
    print(f"âœ“ åŠ è½½äº† {len(texts)} æ¡æ–‡æœ¬")
    
    # ç”Ÿæˆembeddings
    print(f"ç”Ÿæˆembeddings (model={embedding_model})...")
    from sentence_transformers import SentenceTransformer
    
    model = SentenceTransformer(embedding_model)
    embeddings = model.encode(
        texts,
        batch_size=32,
        show_progress_bar=True,
        convert_to_numpy=True,
        normalize_embeddings=True
    )
    
    print(f"âœ“ Embeddings shape: {embeddings.shape}")
    
    return texts, embeddings


def evaluate_clustering(
    embeddings: np.ndarray,
    labels: np.ndarray,
    method_name: str
) -> Dict:
    """
    è¯„ä¼°èšç±»è´¨é‡
    
    æŒ‡æ ‡ï¼š
    1. Silhouette Score (è½®å»“ç³»æ•°) - è¶Šæ¥è¿‘1è¶Šå¥½
    2. Davies-Bouldin Index - è¶Šå°è¶Šå¥½
    3. Calinski-Harabasz Score - è¶Šå¤§è¶Šå¥½
    4. ç°‡å¤§å°åˆ†å¸ƒ
    5. Shannonç†µï¼ˆåˆ†å¸ƒå‡åŒ€æ€§ï¼‰
    """
    from sklearn.metrics import (
        silhouette_score,
        davies_bouldin_score,
        calinski_harabasz_score
    )
    
    print(f"\nè¯„ä¼° {method_name}...")
    
    results = {'method': method_name}
    
    # è¿‡æ»¤å™ªå£°ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰
    valid_mask = labels != -1
    valid_embeddings = embeddings[valid_mask]
    valid_labels = labels[valid_mask]
    
    n_noise = np.sum(~valid_mask)
    n_clusters = len(np.unique(valid_labels))
    
    results['n_clusters'] = n_clusters
    results['n_noise'] = n_noise
    
    # 1. Silhouette Scoreï¼ˆé‡‡æ ·è®¡ç®—ï¼ŒåŠ é€Ÿï¼‰
    try:
        sample_size = min(1000, len(valid_embeddings))
        silhouette = silhouette_score(
            valid_embeddings,
            valid_labels,
            sample_size=sample_size
        )
        results['silhouette'] = float(silhouette)
    except Exception as e:
        print(f"  è­¦å‘Š: æ— æ³•è®¡ç®—Silhouette Score: {e}")
        results['silhouette'] = 0.0
    
    # 2. Davies-Bouldin Index
    try:
        davies_bouldin = davies_bouldin_score(valid_embeddings, valid_labels)
        results['davies_bouldin'] = float(davies_bouldin)
    except Exception as e:
        print(f"  è­¦å‘Š: æ— æ³•è®¡ç®—Davies-Bouldin: {e}")
        results['davies_bouldin'] = 999.0
    
    # 3. Calinski-Harabasz Score
    try:
        calinski = calinski_harabasz_score(valid_embeddings, valid_labels)
        results['calinski_harabasz'] = float(calinski)
    except Exception as e:
        print(f"  è­¦å‘Š: æ— æ³•è®¡ç®—Calinski-Harabasz: {e}")
        results['calinski_harabasz'] = 0.0
    
    # 4. ç°‡å¤§å°åˆ†å¸ƒ
    unique, counts = np.unique(valid_labels, return_counts=True)
    results['cluster_size_min'] = int(counts.min())
    results['cluster_size_max'] = int(counts.max())
    results['cluster_size_mean'] = float(counts.mean())
    results['cluster_size_std'] = float(counts.std())
    
    # 5. Shannonç†µï¼ˆåˆ†å¸ƒå‡åŒ€æ€§ï¼‰
    cluster_probs = counts / counts.sum()
    shannon_entropy = -np.sum(cluster_probs * np.log(cluster_probs + 1e-10))
    max_entropy = np.log(len(counts))
    normalized_entropy = shannon_entropy / max_entropy if max_entropy > 0 else 0
    results['shannon_entropy'] = float(shannon_entropy)
    results['normalized_entropy'] = float(normalized_entropy)
    
    # æ‰“å°ç»“æœ
    print(f"  ç°‡æ•°: {n_clusters}, å™ªå£°ç‚¹: {n_noise}")
    print(f"  è½®å»“ç³»æ•°: {results['silhouette']:.3f} (è¶Šæ¥è¿‘1è¶Šå¥½)")
    print(f"  Davies-Bouldin: {results['davies_bouldin']:.3f} (è¶Šå°è¶Šå¥½)")
    print(f"  Calinski-Harabasz: {results['calinski_harabasz']:.1f} (è¶Šå¤§è¶Šå¥½)")
    print(f"  æ ‡å‡†åŒ–ç†µ: {results['normalized_entropy']:.3f} (è¶Šæ¥è¿‘1è¶Šå¥½)")
    print(f"  ç°‡å¤§å°: {results['cluster_size_min']}-{results['cluster_size_max']} (mean={results['cluster_size_mean']:.1f})")
    
    return results


def compare_methods(
    embeddings: np.ndarray,
    texts: List[str],
    n_clusters: int = 100,
    methods: List[str] = None
) -> Dict[str, Dict]:
    """
    å¯¹æ¯”å¤šç§èšç±»æ–¹æ³•
    
    å‚æ•°:
    - embeddings: embeddingçŸ©é˜µ
    - texts: æ–‡æœ¬åˆ—è¡¨
    - n_clusters: ç›®æ ‡ç°‡æ•°
    - methods: è¦å¯¹æ¯”çš„æ–¹æ³•åˆ—è¡¨
    
    è¿”å›:
    - results: {method_name: evaluation_results}
    """
    if methods is None:
        methods = ['kmeans', 'hybrid']  # é»˜è®¤å¯¹æ¯”è¿™ä¸¤ç§
    
    print("=" * 70)
    print(f"èšç±»æ–¹æ³•å¯¹æ¯”å®éªŒ")
    print(f"æ•°æ®é‡: {len(embeddings)}")
    print(f"ç›®æ ‡ç°‡æ•°: {n_clusters}")
    print(f"å¯¹æ¯”æ–¹æ³•: {', '.join(methods)}")
    print("=" * 70)
    
    results = {}
    
    for method in methods:
        print(f"\n{'='*70}")
        print(f"æ–¹æ³•: {method.upper()}")
        print('='*70)
        
        # è®°å½•æ—¶é—´
        start_time = time.time()
        
        try:
            # å¯¼å…¥å¹¶è¿è¡Œèšç±»
            if method == 'kmeans':
                from sklearn.cluster import MiniBatchKMeans
                clusterer = MiniBatchKMeans(
                    n_clusters=n_clusters,
                    random_state=42,
                    batch_size=1000
                )
                labels = clusterer.fit_predict(embeddings)
            
            elif method == 'hybrid':
                from improved_clustering_sampler import ImprovedDiversitySampler
                sampler = ImprovedDiversitySampler(
                    method='hybrid',
                    n_clusters=n_clusters
                )
                labels = sampler.fit_predict(embeddings)
            
            elif method == 'hdbscan':
                try:
                    import hdbscan
                    clusterer = hdbscan.HDBSCAN(
                        min_cluster_size=15,
                        min_samples=5,
                        metric='cosine',
                        cluster_selection_method='eom'
                    )
                    labels = clusterer.fit_predict(embeddings)
                except ImportError:
                    print("  âš ï¸ æœªå®‰è£…hdbscanï¼Œè·³è¿‡")
                    print("  å®‰è£…: pip install hdbscan")
                    continue
            
            elif method == 'agglomerative':
                from sklearn.cluster import AgglomerativeClustering
                # å¯¹å¤§æ•°æ®ä½¿ç”¨ä¸¤é˜¶æ®µèšç±»
                if len(embeddings) > 10000:
                    print("  ä½¿ç”¨ä¸¤é˜¶æ®µèšç±»ï¼ˆæ•°æ®é‡å¤§ï¼‰")
                    # é˜¶æ®µ1: K-meansç²—èšç±»
                    from sklearn.cluster import MiniBatchKMeans
                    kmeans = MiniBatchKMeans(n_clusters=min(n_clusters*10, len(embeddings)//2))
                    coarse_labels = kmeans.fit_predict(embeddings)
                    # é˜¶æ®µ2: å¯¹ç°‡ä¸­å¿ƒèšç±»
                    centers = kmeans.cluster_centers_
                    clusterer = AgglomerativeClustering(
                        n_clusters=n_clusters,
                        metric='cosine',
                        linkage='average'
                    )
                    center_labels = clusterer.fit_predict(centers)
                    labels = center_labels[coarse_labels]
                else:
                    clusterer = AgglomerativeClustering(
                        n_clusters=n_clusters,
                        metric='cosine',
                        linkage='average'
                    )
                    labels = clusterer.fit_predict(embeddings)
            
            else:
                print(f"  âš ï¸ æœªçŸ¥æ–¹æ³•: {method}")
                continue
            
            # è®°å½•æ—¶é—´
            elapsed_time = time.time() - start_time
            
            # è¯„ä¼°
            result = evaluate_clustering(embeddings, labels, method)
            result['time'] = elapsed_time
            
            results[method] = result
            
            print(f"  âœ“ å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.1f}ç§’")
        
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    return results


def print_comparison_summary(results: Dict[str, Dict]):
    """æ‰“å°å¯¹æ¯”æ€»ç»“"""
    print("\n" + "=" * 70)
    print("å¯¹æ¯”æ€»ç»“")
    print("=" * 70)
    
    if len(results) == 0:
        print("æ²¡æœ‰æˆåŠŸçš„å®éªŒç»“æœ")
        return
    
    # åˆ›å»ºå¯¹æ¯”è¡¨
    print(f"\n{'æ–¹æ³•':<15} {'ç°‡æ•°':<8} {'è½®å»“ç³»æ•°':<12} {'æ ‡å‡†åŒ–ç†µ':<12} {'æ—¶é—´(ç§’)':<10}")
    print("-" * 70)
    
    for method, result in results.items():
        print(f"{method:<15} "
              f"{result['n_clusters']:<8} "
              f"{result['silhouette']:<12.3f} "
              f"{result['normalized_entropy']:<12.3f} "
              f"{result['time']:<10.1f}")
    
    # æ¨è
    print("\n" + "=" * 70)
    print("æ¨è")
    print("=" * 70)
    
    # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
    best_quality = max(results.items(), key=lambda x: x[1]['silhouette'])
    best_diversity = max(results.items(), key=lambda x: x[1]['normalized_entropy'])
    fastest = min(results.items(), key=lambda x: x[1]['time'])
    
    print(f"\næœ€é«˜è´¨é‡ï¼ˆè½®å»“ç³»æ•°ï¼‰: {best_quality[0]} ({best_quality[1]['silhouette']:.3f})")
    print(f"æœ€é«˜å¤šæ ·æ€§ï¼ˆæ ‡å‡†åŒ–ç†µï¼‰: {best_diversity[0]} ({best_diversity[1]['normalized_entropy']:.3f})")
    print(f"æœ€å¿«é€Ÿåº¦: {fastest[0]} ({fastest[1]['time']:.1f}ç§’)")
    
    # ç»¼åˆæ¨è
    print("\nğŸ’¡ ç»¼åˆæ¨è:")
    
    if 'hybrid' in results:
        hybrid_result = results['hybrid']
        print(f"  âœ¨ Hybridæ–¹æ³• - å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡")
        print(f"     è½®å»“ç³»æ•°: {hybrid_result['silhouette']:.3f}")
        print(f"     æ ‡å‡†åŒ–ç†µ: {hybrid_result['normalized_entropy']:.3f}")
        print(f"     æ—¶é—´: {hybrid_result['time']:.1f}ç§’")
        
        if hybrid_result['silhouette'] > 0.25 and hybrid_result['normalized_entropy'] > 0.75:
            print(f"     çŠ¶æ€: âœ… è´¨é‡å’Œå¤šæ ·æ€§éƒ½å¾ˆå¥½")
        else:
            print(f"     çŠ¶æ€: âš ï¸ å¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°")
    
    if 'hdbscan' in results:
        hdbscan_result = results['hdbscan']
        print(f"\n  ğŸ¯ HDBSCAN - è¿½æ±‚æè‡´ç²¾åº¦")
        print(f"     è½®å»“ç³»æ•°: {hdbscan_result['silhouette']:.3f}")
        print(f"     æ ‡å‡†åŒ–ç†µ: {hdbscan_result['normalized_entropy']:.3f}")
        print(f"     æ—¶é—´: {hdbscan_result['time']:.1f}ç§’")
        
        if hdbscan_result['silhouette'] > best_quality[1]['silhouette'] * 1.05:
            print(f"     çŠ¶æ€: âœ… æ¯”K-meansæ˜¾è‘—æ›´å¥½ï¼Œå»ºè®®ä½¿ç”¨")
        else:
            print(f"     çŠ¶æ€: âš ï¸ æå‡æœ‰é™ï¼Œå¯æ ¹æ®æ—¶é—´æˆæœ¬å†³å®š")
    
    # ç»™å‡ºæœ€ç»ˆå»ºè®®
    print("\nğŸ¯ æœ€ç»ˆå»ºè®®:")
    
    if 'kmeans' in results and 'hybrid' in results:
        kmeans_score = results['kmeans']['silhouette']
        hybrid_score = results['hybrid']['silhouette']
        improvement = (hybrid_score - kmeans_score) / kmeans_score * 100
        
        if improvement > 10:
            print(f"  âœ… Hybridæ¯”K-meansæå‡ {improvement:.1f}%ï¼Œå¼ºçƒˆå»ºè®®å‡çº§")
        elif improvement > 5:
            print(f"  âœ… Hybridæ¯”K-meansæå‡ {improvement:.1f}%ï¼Œå»ºè®®å‡çº§")
        else:
            print(f"  âš ï¸ Hybridæå‡ä¸æ˜æ˜¾ï¼ˆ{improvement:.1f}%ï¼‰ï¼Œç»§ç»­ä½¿ç”¨K-meansä¹Ÿå¯")
    
    print("\n  æ ¹æ®æ‚¨çš„ç›®æ ‡é€‰æ‹©:")
    print("    - é€Ÿåº¦ä¼˜å…ˆ â†’ K-means")
    print("    - å¹³è¡¡æ–¹æ¡ˆ â†’ Hybridï¼ˆæ¨èï¼‰")
    print("    - è´¨é‡ä¼˜å…ˆ â†’ HDBSCAN")


def save_results(results: Dict[str, Dict], output_file: str):
    """ä¿å­˜ç»“æœåˆ°JSON"""
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, indent=2, fp=f, ensure_ascii=False)
    
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description="èšç±»æ–¹æ³•å¯¹æ¯”å®éªŒ",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="è¾“å…¥æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--n_samples",
        type=int,
        default=5000,
        help="æµ‹è¯•æ ·æœ¬æ•°ï¼ˆå»ºè®®5000-10000ï¼‰"
    )
    parser.add_argument(
        "--n_clusters",
        type=int,
        default=100,
        help="ç›®æ ‡ç°‡æ•°"
    )
    parser.add_argument(
        "--methods",
        type=str,
        default="kmeans,hybrid",
        help="å¯¹æ¯”çš„æ–¹æ³•ï¼Œé€—å·åˆ†éš” (kmeans,hybrid,hdbscan,agglomerative)"
    )
    parser.add_argument(
        "--embedding_model",
        type=str,
        default="moka-ai/m3e-base",
        help="Embeddingæ¨¡å‹"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="clustering_comparison_results.json",
        help="ç»“æœè¾“å‡ºæ–‡ä»¶"
    )
    
    args = parser.parse_args()
    
    # è§£ææ–¹æ³•åˆ—è¡¨
    methods = [m.strip() for m in args.methods.split(',')]
    
    # åŠ è½½æ•°æ®
    texts, embeddings = load_and_prepare_data(
        args.input,
        args.n_samples,
        args.embedding_model
    )
    
    # å¯¹æ¯”å®éªŒ
    results = compare_methods(
        embeddings,
        texts,
        args.n_clusters,
        methods
    )
    
    # æ‰“å°æ€»ç»“
    print_comparison_summary(results)
    
    # ä¿å­˜ç»“æœ
    save_results(results, args.output)


if __name__ == "__main__":
    main()

